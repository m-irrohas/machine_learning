{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:OFF \n",
      "Load_Network:ON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/develop/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "/home/develop/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages/ipykernel_launcher.py:226: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"le..., outputs=Tensor(\"le...)`\n",
      "/home/develop/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages/ipykernel_launcher.py:458: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5460b9cd6632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0mleaner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m     \u001b[0mleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobtain_q_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-5460b9cd6632>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, number, sess)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mtarget_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbubble_sort_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"learner_target_parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobtain_q_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mq_network_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobtain_target_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget_network_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-5460b9cd6632>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mtarget_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbubble_sort_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"learner_target_parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobtain_q_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mq_network_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobtain_target_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget_network_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Flatten, Dense, Input, Lambda, concatenate\n",
    "from keras import backend as K\n",
    "import time\n",
    "from gym import wrappers\n",
    "import threading\n",
    "\n",
    "#GPUの使用率の設定\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "#Environment\n",
    "ENV_NAME = 'Breakout-v0'  # 環境を記述(gym.makeより)\n",
    "\n",
    "#TrainとLoadの使用の有無\n",
    "TRAIN = False #trainするか？\n",
    "LOAD_NETWORK = True #networkをロードするか\n",
    "SAVE_NETWORK_PATH = 'saved_networks/' + ENV_NAME #saveするディレクトリ\n",
    "\n",
    "#諸パラメータ\n",
    "NUM_ACTORS = 1 #actorの数\n",
    "NUM_EPISODES = 120  # エピソードの総数\n",
    "INITIAL_REPLAY_SIZE = 500  # learnerが待つ数\n",
    "NUM_REPLAY_MEMORY = 2000  # メモリ数\n",
    "MEMORY_REMOVE_INTERVAL = 100 #メモリのインターバル\n",
    "PARAMETER_COPY_INTERVAL = 400 #パラメータのインターバル\n",
    "EPSILON_EXPOENT_ALPHA = 7 #e-greedyのアルファ\n",
    "EPSILON = 0.4 #e-greedyのe\n",
    "SEND_BATCH_SIZE = 50 #batchsize\n",
    "PRINT_INTERVAL = 300\n",
    "N_STEP_RETURN = 3 #n-stepのターゲット\n",
    "GAMMA = 0.99  # 割引率\n",
    "GAMMA_N = GAMMA ** N_STEP_RETURN #n-step考慮した割引率\n",
    "PRIORITY_ALPHA = 0.6 #優先度\n",
    "\n",
    "# About epsilon-greedy\n",
    "ANEALING_EPSILON = True\n",
    "EXPLORATION_STEPS = 10000  # 探査数\n",
    "INITIAL_EPSILON = 1.0  # epsilon初期値\n",
    "FINAL_EPSILON = 0.1  # epsilon終値(これ以上は下がらない)\n",
    "\n",
    "#rendeing\n",
    "FRAME_WIDTH = 84  # Resized frame width\n",
    "FRAME_HEIGHT = 84  # Resized frame height\n",
    "#network\n",
    "STATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\n",
    "BATCH_SIZE = 32  # Mini batch size, 512 is the best.\n",
    "TARGET_UPDATE_INTERVAL = 250 # The frequency with which the target network is updated\n",
    "ACTION_INTERVAL = 4  # The agent sees only every () input\n",
    "LEARNING_RATE = 0.00025 / 4  # 学習率\n",
    "SAVE_INTERVAL = 500  # The frequency with which the network is saved\n",
    "NO_OP_STEPS = 30  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\n",
    "NUM_EPISODES_AT_TEST = 10  # Number of episodes the agent plays at test time\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"\n",
    "    優先順位付き経験再生のクラス\n",
    "    メモリー部分\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.transition = deque() #リモートメモリ\n",
    "        self.priorities = deque() #ローカルメモリ\n",
    "        self.total_p = 0 #正規化用\n",
    "\n",
    "    def _error_to_priority(self, error_batch):\n",
    "        priority_batch = []\n",
    "        for error in error_batch:\n",
    "            priority_batch.append(error**PRIORITY_ALPHA) #TD誤差を数列化←sumapp\n",
    "        return priority_batch\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.transition)\n",
    "\n",
    "    def add(self, transiton_batch, error_batch):\n",
    "        priority_batch = self._error_to_priority(error_batch)\n",
    "        self.total_p += sum(priority_batch) \n",
    "        self.transition.extend(transiton_batch) #リストの結合なのでextendになることに注意\n",
    "        self.priorities.extend(priority_batch)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idx_batch = []\n",
    "        segment = self.total_p / n #nに対する区間\n",
    "\n",
    "        idx = -1 # index\n",
    "        sum_p = 0 #sumapp\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            while sum_p < s:\n",
    "                sum_p += self.priorities[idx]\n",
    "                idx += 1\n",
    "            idx_batch.append(idx)\n",
    "            batch.append(self.transition[idx])\n",
    "        return batch, idx_batch\n",
    "\n",
    "\n",
    "    def update(self, idx_batch, error_batch):\n",
    "        priority_batch = self._error_to_priority(error_batch)\n",
    "        for i in range(len(idx_batch)):\n",
    "            change = priority_batch[i] - self.priorities[idx_batch[i]]\n",
    "            self.total_p += change\n",
    "            self.priorities[idx_batch[i]] = priority_batch[i]\n",
    "\n",
    "\n",
    "    def remove(self):\n",
    "        print(\"Excess Memory: \", (len(self.priorities) - NUM_REPLAY_MEMORY))\n",
    "        for _ in range(len(self.priorities) - NUM_REPLAY_MEMORY):\n",
    "            self.transition.popleft()\n",
    "            p = self.priorities.popleft()\n",
    "            self.total_p -= p\n",
    "\n",
    "\n",
    "\n",
    "class Learner:\n",
    "    \"\"\"\n",
    "    Learnerクラス\n",
    "    DNNにtfを用いる\n",
    "    \"\"\"\n",
    "    def __init__(self, sess):\n",
    "        self.sess = sess\n",
    "        self.f_end = False\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        self.t = 0\n",
    "        self.total_time = 0\n",
    "\n",
    "        # Parameters used for summary\n",
    "        self.total_reward = 0\n",
    "        self.total_q_max = 0\n",
    "        self.total_loss = 0\n",
    "        self.duration = 0\n",
    "        self.episode = 0\n",
    "\n",
    "        self.start = 0\n",
    "\n",
    "        with tf.variable_scope(\"learner_parameters\", reuse=True):\n",
    "            self.s, self.q_values, q_network = self.build_network()\n",
    "        q_network_weights = self.bubble_sort_parameters(q_network.trainable_weights)\n",
    "\n",
    "        # Create target network\n",
    "        with tf.variable_scope(\"learner_target_parameters\", reuse=True):\n",
    "            self.st, self.target_q_values, target_network = self.build_network()\n",
    "        target_network_weights = self.bubble_sort_parameters(target_network.trainable_weights)\n",
    "\n",
    "        # Define target network update operation\n",
    "        self.update_target_network = [target_network_weights[i].assign(q_network_weights[i]) for i in range(len(target_network_weights))]\n",
    "\n",
    "\n",
    "        # Define loss and gradient update operation\n",
    "        self.a, self.y, self.error, self.loss, self.grad_update, self.gv, self.cl = self.build_training_op(q_network_weights)\n",
    "\n",
    "\n",
    "\n",
    "        #paths接続\n",
    "        if not os.path.exists(SAVE_NETWORK_PATH):\n",
    "            os.makedirs(SAVE_NETWORK_PATH)\n",
    "        \n",
    "        self.saver = tf.train.Saver(q_network_weights)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Initialize target network\n",
    "        self.sess.run(self.update_target_network)\n",
    "\n",
    "\n",
    "    def bubble_sort_parameters(self, arr):\n",
    "        \"\"\"バブルソート\n",
    "        \n",
    "        Arg : \n",
    "            arr(list) リスト\n",
    "        \n",
    "        Attribute : \n",
    "            arr(list) ソートされたリスト\"\"\"\n",
    "        \n",
    "        change = True\n",
    "        while change:\n",
    "            change = False\n",
    "            for i in range(len(arr) - 1):\n",
    "                if arr[i].name > arr[i + 1].name:\n",
    "                    arr[i], arr[i + 1] = arr[i + 1], arr[i]\n",
    "                    change = True\n",
    "        return arr\n",
    "\n",
    "\n",
    "    def build_network(self):\n",
    "        \"\"\"build network\n",
    "        \n",
    "        Arg :\n",
    "            特になし\n",
    "            \n",
    "        Attributes :\n",
    "            s : PlaceHolder\n",
    "            q_values : 行動価値関数(model構築済み)\n",
    "            model : モデル\"\"\"\n",
    "        #Dual Networkの構築\n",
    "        #kerasが便利\n",
    "        l_input = Input(shape=(4,84,84))\n",
    "        conv2d = Conv2D(32,8,strides=(4,4),activation='relu', data_format=\"channels_first\")(l_input)\n",
    "        conv2d = Conv2D(64,4,strides=(2,2),activation='relu', data_format=\"channels_first\")(conv2d)\n",
    "        conv2d = Conv2D(64,3,strides=(1,1),activation='relu', data_format=\"channels_first\")(conv2d)\n",
    "        fltn = Flatten()(conv2d) #入力の平滑化\n",
    "        v = Dense(512, activation='relu', name=\"dense_v1\")(fltn)\n",
    "        v = Dense(1, name=\"dense_v2\")(v)\n",
    "        adv = Dense(512, activation='relu', name=\"dense_adv1\")(fltn)\n",
    "        adv = Dense(self.num_actions, name=\"dense_adv2\")(adv)\n",
    "        y = concatenate([v,adv]) #層を連結\n",
    "        l_output = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - tf.stop_gradient(K.mean(a[:,1:],keepdims=True)), output_shape=(self.num_actions,))(y)\n",
    "        \n",
    "        model = Model(input=l_input,output=l_output)\n",
    "        s = tf.placeholder(tf.float32, [None, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT]) #プレースホルダー\n",
    "        q_values = model(s)\n",
    "\n",
    "        return s, q_values, model\n",
    "\n",
    "    def build_training_op(self, q_network_weights):\n",
    "        \"\"\"trainの構築\n",
    "        \n",
    "        Arg :\n",
    "            q_network_wights\"\"\"\n",
    "        a = tf.placeholder(tf.int64, [None])\n",
    "        y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        # Convert action to one hot vector. shape=(BATCH_SIZE, num_actions)\n",
    "        a_one_hot = tf.one_hot(a, self.num_actions, 1.0, 0.0)\n",
    "        # shape = (BATCH_SIZE,)\n",
    "        q_value = tf.reduce_sum(tf.multiply(self.q_values, a_one_hot), reduction_indices=1)\n",
    "\n",
    "        # Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n",
    "        error = tf.abs(y - q_value)\n",
    "        # error_is = (w / tf.reduce_max(w)) * error\n",
    "        quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, decay=0.95, epsilon=1.5e-7, centered=True)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss, var_list=q_network_weights)\n",
    "        capped_gvs = [(grad if grad is None else tf.clip_by_norm(grad, clip_norm=40), var) for grad, var in grads_and_vars]\n",
    "        grad_update = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "        return a, y, error, loss, grad_update ,grads_and_vars, capped_gvs\n",
    "\n",
    "    def load_network(self):\n",
    "        checkpoint = tf.train.get_checkpoint_state(SAVE_NETWORK_PATH)\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
    "            print('Successfully loaded: ' + checkpoint.model_checkpoint_path)\n",
    "        else:\n",
    "            print('Training new network...')\n",
    "\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        global total_episode\n",
    "\n",
    "        # This should be done after Actors were generated.\n",
    "        if LOAD_NETWORK:\n",
    "            self.load_network()\n",
    "\n",
    "        if remote_memory.length() < INITIAL_REPLAY_SIZE:\n",
    "            print(\"Learner Waiting...\")\n",
    "            time.sleep(10)\n",
    "            self.run()\n",
    "\n",
    "        if not self.f_end:\n",
    "            print(\"Learner Starts!\")\n",
    "\n",
    "\n",
    "        while not self.f_end:\n",
    "            start = time.time()\n",
    "\n",
    "            state_batch = [] #状態\n",
    "            action_batch = [] #行動\n",
    "            reward_batch = [] #報酬\n",
    "            next_state_batch = [] #次の状態\n",
    "            terminal_batch = []\n",
    "\n",
    "            minibatch, idx_batch = remote_memory.sample(BATCH_SIZE)\n",
    "\n",
    "            for data in minibatch:\n",
    "                state_batch.append(data[0])\n",
    "                action_batch.append(data[1])\n",
    "                reward_batch.append(data[2])\n",
    "                #shape = (BATCH_SIZE, 4, 32, 32)\n",
    "                next_state_batch.append(data[3])\n",
    "                terminal_batch.append(data[4])\n",
    "\n",
    "                self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [np.float32(data[0] / 255.0)]},session=self.sess))\n",
    "\n",
    "            # Convert True to 1, False to 0\n",
    "            terminal_batch = np.array(terminal_batch) + 0\n",
    "            # shape = (BATCH_SIZE, num_actions)\n",
    "            target_q_values_batch = self.target_q_values.eval(feed_dict={self.st: np.float32(np.array(next_state_batch) / 255.0)}, session=self.sess)\n",
    "            # DDQN\n",
    "            actions = np.argmax(self.q_values.eval(feed_dict={self.s: np.float32(np.array(next_state_batch) / 255.0)}, session=self.sess), axis=1)\n",
    "            target_q_values_batch = np.array([target_q_values_batch[i][action] for i, action in enumerate(actions)])\n",
    "            # shape = (BATCH_SIZE,)\n",
    "            y_batch = reward_batch + (1 - terminal_batch) * GAMMA_N * target_q_values_batch\n",
    "\n",
    "\n",
    "            error_batch = self.error.eval(feed_dict={\n",
    "                self.s: np.float32(np.array(state_batch) / 255.0),\n",
    "                self.a: action_batch,\n",
    "                self.y: y_batch\n",
    "            }, session=self.sess)\n",
    "\n",
    "            loss, _ = self.sess.run([self.loss, self.grad_update], feed_dict={\n",
    "                self.s: np.float32(np.array(state_batch) / 255.0),\n",
    "                self.a: action_batch,\n",
    "                self.y: y_batch\n",
    "            })\n",
    "\n",
    "\n",
    "            self.total_loss += loss\n",
    "            self.total_time += time.time() - start\n",
    "\n",
    "            # Memory update\n",
    "            remote_memory.update(idx_batch, error_batch)\n",
    "\n",
    "            self.t += 1\n",
    "\n",
    "            if self.t % PRINT_INTERVAL == 0:\n",
    "                text_l = 'AVERAGE LOSS: {0:.5F} / AVG_MAX_Q: {1:2.4F} / LEARN PER SECOND: {2:.1F} / NUM LEARN: {3:5d}'.format(\n",
    "                    self.total_loss/PRINT_INTERVAL, self.total_q_max/(PRINT_INTERVAL*BATCH_SIZE), PRINT_INTERVAL/self.total_time, self.t)\n",
    "                print(text_l)\n",
    "                with open(ENV_NAME+'_output.txt','a') as f:\n",
    "                    f.write(text_l+\"\\n\")\n",
    "                self.total_loss = 0\n",
    "                self.total_time = 0\n",
    "                self.total_q_max = 0\n",
    "\n",
    "            # Remove excess memory\n",
    "            if self.t % MEMORY_REMOVE_INTERVAL == 0 and remote_memory.length() > NUM_REPLAY_MEMORY:\n",
    "                remote_memory.remove()\n",
    "\n",
    "            # Update target network\n",
    "            if self.t % TARGET_UPDATE_INTERVAL == 0:\n",
    "                self.sess.run(self.update_target_network)\n",
    "\n",
    "            # Save network\n",
    "            if self.t % SAVE_INTERVAL == 0:\n",
    "                save_path = self.saver.save(self.sess, SAVE_NETWORK_PATH + '/' + ENV_NAME, global_step=(self.t))\n",
    "                print('Successfully saved: ' + save_path)\n",
    "\n",
    "            if total_episode >= NUM_EPISODES:\n",
    "                self.f_end = True\n",
    "\n",
    "        print(\"The Learning is Over.\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, number, sess):\n",
    "        self.sess = sess\n",
    "        self.f_end = False\n",
    "\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "\n",
    "        self.num = number\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.t = 0\n",
    "        self.repeated_action = 0\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.total_q_max = 0\n",
    "        self.total_loss = 0\n",
    "        self.duration = 0\n",
    "        self.episode = 0\n",
    "\n",
    "        if NUM_ACTORS != 1:\n",
    "            self.epsilon = EPSILON **(1+(self.num/(NUM_ACTORS-1))*EPSILON_EXPOENT_ALPHA)\n",
    "        else:\n",
    "            self.epsilon = EPSILON\n",
    "\n",
    "\n",
    "        if ANEALING_EPSILON:\n",
    "            self.epsilon = INITIAL_EPSILON\n",
    "            self.epsilon_step = (INITIAL_EPSILON -FINAL_EPSILON)/ EXPLORATION_STEPS\n",
    "\n",
    "\n",
    "        self.local_memory = deque(maxlen=100)\n",
    "        self.buffer = []\n",
    "        self.R = 0\n",
    "\n",
    "        self.s, self.q_values, q_network = self.build_network()\n",
    "        q_network_weights = self.bubble_sort_parameters(q_network.trainable_weights)\n",
    "        \n",
    "        self.st, self.target_q_values, target_network = self.build_network()\n",
    "        target_network_weights = self.bubble_sort_parameters(target_network.trainable_weights)\n",
    "        \n",
    "        q_parameters = self.bubble_sort_parameters(tf.trainable_variables(scope=\"learner_parameters\"))\n",
    "        target_parameters = self.bubble_sort_parameters(tf.trainable_variables(scope=\"learner_target_parameters\"))\n",
    "        \n",
    "        self.obtain_q_parameters = [q_network_weights[i].assign(q_parameters[i]) for i in range(len(q_parameters))]\n",
    "        self.obtain_target_parameters = [target_network_weights[i].assign(target_parameters[i]) for i in range(len(target_parameters))]\n",
    "\n",
    "        self.a, self.y, self.q, self.error = self.td_error_op()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def bubble_sort_parameters(self, arr):\n",
    "        # sort\n",
    "        change = True\n",
    "        while change:\n",
    "            change = False\n",
    "            for i in range(len(arr) - 1):\n",
    "                if arr[i].name > arr[i + 1].name:\n",
    "                    arr[i], arr[i + 1] = arr[i + 1], arr[i]\n",
    "                    change = True\n",
    "        return arr\n",
    "\n",
    "\n",
    "    def td_error_op(self):\n",
    "        a = tf.placeholder(tf.int64, [None])\n",
    "        y = tf.placeholder(tf.float32, [None])\n",
    "        q = tf.placeholder(tf.float32, [None,None])\n",
    "\n",
    "        # Convert action to one hot vector. shape=(BATCH_SIZE, num_actions)\n",
    "        a_one_hot = tf.one_hot(a, self.num_actions, 1.0, 0.0)\n",
    "        # shape = (BATCH_SIZE,)\n",
    "        q_value = tf.reduce_sum(tf.multiply(q, a_one_hot), reduction_indices=1)\n",
    "\n",
    "        # Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n",
    "        error = tf.abs(y - q_value)\n",
    "\n",
    "        return a, y, q, error\n",
    "\n",
    "\n",
    "    def build_network(self):\n",
    "        l_input = Input(shape=(4,84,84))\n",
    "        conv2d = Conv2D(32,8,strides=(4,4),activation='relu', data_format=\"channels_first\")(l_input)\n",
    "        conv2d = Conv2D(64,4,strides=(2,2),activation='relu', data_format=\"channels_first\")(conv2d)\n",
    "        conv2d = Conv2D(64,3,strides=(1,1),activation='relu', data_format=\"channels_first\")(conv2d)\n",
    "        fltn = Flatten()(conv2d)\n",
    "        v = Dense(512, activation='relu', name=\"dense_v1_\"+str(self.num))(fltn)\n",
    "        v = Dense(1, name=\"dense_v2_\"+str(self.num))(v)\n",
    "        adv = Dense(512, activation='relu', name=\"dense_adv1_\"+str(self.num))(fltn)\n",
    "        adv = Dense(self.num_actions, name=\"dense_adv2_\"+str(self.num))(adv)\n",
    "        y = concatenate([v,adv])\n",
    "        l_output = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - tf.stop_gradient(K.mean(a[:,1:],keepdims=True)), output_shape=(self.num_actions,))(y)\n",
    "        model = Model(input=l_input,output=l_output)\n",
    "\n",
    "        s = tf.placeholder(tf.float32, [None, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT])\n",
    "        q_values = model(s)\n",
    "\n",
    "        return s, q_values, model\n",
    "\n",
    "    def get_initial_state(self, observation, last_observation):\n",
    "        processed_observation = np.maximum(observation, last_observation)\n",
    "        processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "        state = [processed_observation for _ in range(STATE_LENGTH)]\n",
    "        return np.stack(state, axis=0)\n",
    "\n",
    "\n",
    "    def preprocess(self, observation, last_observation):\n",
    "        processed_observation = np.maximum(observation, last_observation)\n",
    "        processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "        return np.reshape(processed_observation, (1, FRAME_WIDTH, FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "\n",
    "    def get_action_and_q(self, state):\n",
    "        action = self.repeated_action\n",
    "        q = self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}, session=self.sess)\n",
    "        if self.t % ACTION_INTERVAL == 0:\n",
    "            if self.epsilon >= random.random() or self.t < INITIAL_REPLAY_SIZE:\n",
    "                action = random.randrange(self.num_actions)\n",
    "            else:\n",
    "                action = np.argmax(q[0])\n",
    "            self.repeated_action = action\n",
    "        return action, q[0]\n",
    "\n",
    "    def get_action_at_test(self, state):\n",
    "        action = self.repeated_action\n",
    "\n",
    "        if self.t % ACTION_INTERVAL == 0:\n",
    "            if random.random() <= 0.05:\n",
    "                action = random.randrange(self.num_actions)\n",
    "            else:\n",
    "                action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "            self.repeated_action = action\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_sample(self, n):\n",
    "        s, a, _, _, _, q = self.buffer[0]\n",
    "        _, _, _, s_, done, q_ = self.buffer[n-1]\n",
    "\n",
    "        return s, a, self.R, s_, done, q, q_\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        global total_episode\n",
    "\n",
    "        if TRAIN:  # Train mode\n",
    "            while not self.f_end:\n",
    "                terminal = False\n",
    "                observation = self.env.reset()\n",
    "                for _ in range(random.randint(1, NO_OP_STEPS)):\n",
    "                    last_observation = observation\n",
    "                    observation, _, _, _ = self.env.step(0)  # Do nothing\n",
    "                state = self.get_initial_state(observation, last_observation)\n",
    "                start = time.time()\n",
    "                while not terminal:\n",
    "                    last_observation = observation\n",
    "                    action, q = self.get_action_and_q(state)\n",
    "                    observation, reward, terminal, _ = self.env.step(action)\n",
    "                    reward = np.sign(reward)\n",
    "                    #env.render()\n",
    "                    processed_observation = self.preprocess(observation, last_observation)\n",
    "                    next_state = np.append(state[1:, :, :], processed_observation, axis=0)\n",
    "\n",
    "\n",
    "                    self.buffer.append((state, action, reward, next_state, terminal, q))\n",
    "                    self.R = (self.R + reward * GAMMA_N) / GAMMA\n",
    "\n",
    "                    # n-step transition\n",
    "                    if terminal:      # terminal state\n",
    "                        while len(self.buffer) > 0:\n",
    "                            n = len(self.buffer)\n",
    "                            s, a, r, s_, done, q, q_ =  self.get_sample(n)\n",
    "                            self.local_memory.append((s, a, r, s_, done, q, q_))\n",
    "                            self.R = (self.R - self.buffer[0][2]) / GAMMA\n",
    "                            self.buffer.pop(0)\n",
    "                        self.R = 0\n",
    "\n",
    "                    if len(self.buffer) >= N_STEP_RETURN:\n",
    "                        s, a, r, s_, done, q, q_ = self.get_sample(N_STEP_RETURN)\n",
    "                        self.local_memory.append((s, a, r, s_, done, q, q_))\n",
    "                        self.R = self.R - self.buffer[0][2]\n",
    "                        self.buffer.pop(0)\n",
    "\n",
    "                    # Add experience and priority to remote memory\n",
    "                    if len(self.local_memory) > 50:\n",
    "                        state_batch = []\n",
    "                        action_batch = []\n",
    "                        reward_batch = []\n",
    "                        next_state_batch = []\n",
    "                        terminal_batch = []\n",
    "                        q_batch = []\n",
    "                        qn_batch = []\n",
    "\n",
    "                        for _ in range(SEND_BATCH_SIZE):\n",
    "                            data = self.local_memory.popleft()\n",
    "                            state_batch.append(data[0])\n",
    "                            action_batch.append(data[1])\n",
    "                            reward_batch.append(data[2])\n",
    "                            #shape = (BATCH_SIZE, 4, 32, 32)\n",
    "                            next_state_batch.append(data[3])\n",
    "                            terminal_batch.append(data[4])\n",
    "                            q_batch.append(data[5])\n",
    "                            qn_batch.append(data[6])\n",
    "\n",
    "                        terminal_batch = np.array(terminal_batch) + 0\n",
    "                        # shape = (BATCH_SIZE, num_actions)\n",
    "                        target_q_values_batch = self.target_q_values.eval(feed_dict={self.st: np.float32(np.array(next_state_batch) / 255.0)}, session=self.sess)\n",
    "                        # DDQN\n",
    "                        actions = np.argmax(qn_batch, axis=1)\n",
    "                        target_q_values_batch = np.array([target_q_values_batch[i][action] for i, action in enumerate(actions)])\n",
    "                        # shape = (BATCH_SIZE,)\n",
    "                        y_batch = reward_batch + (1 - terminal_batch) * GAMMA_N * target_q_values_batch\n",
    "\n",
    "                        error_batch = self.error.eval(feed_dict={\n",
    "                            self.s: np.float32(np.array(state_batch) / 255.0),\n",
    "                            self.a: action_batch,\n",
    "                            self.q: q_batch,\n",
    "                            self.y: y_batch\n",
    "                        }, session=self.sess)\n",
    "\n",
    "                        send = [(state_batch[i],action_batch[i],reward_batch[i],next_state_batch[i],terminal_batch[i]) for i in range(SEND_BATCH_SIZE)]\n",
    "\n",
    "                        remote_memory.add(send, error_batch)\n",
    "\n",
    "                    state = next_state\n",
    "\n",
    "                    self.t += 1\n",
    "\n",
    "                    if self.t % PARAMETER_COPY_INTERVAL == 0:\n",
    "                        self.sess.run(self.obtain_q_parameters)\n",
    "                        self.sess.run(self.obtain_target_parameters)\n",
    "\n",
    "                    if ANEALING_EPSILON and EXPLORATION_STEPS + INITIAL_REPLAY_SIZE > self.t >= INITIAL_REPLAY_SIZE:\n",
    "                        self.epsilon -= self.epsilon_step\n",
    "\n",
    "                    self.total_reward += reward\n",
    "                    self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}, session=self.sess))\n",
    "                    self.duration += 1\n",
    "\n",
    "                elapsed = time.time() - start\n",
    "\n",
    "                text = 'EPISODE: {0:6d} / ACTOR: {1:3d} / TIMESTEP: {2:8d} / DURATION: {3:5d} / EPSILON: {4:.5f} / TOTAL_REWARD: {5:3.0f} / AVG_MAX_Q: {6:2.4f} / STEP_PER_SECOND: {7:.1f}'.format(\n",
    "                    self.episode + 1, self.num, self.t, self.duration, self.epsilon,\n",
    "                    self.total_reward, self.total_q_max / float(self.duration),\n",
    "                    self.duration/elapsed)\n",
    "\n",
    "                print(text)\n",
    "\n",
    "\n",
    "                with open(ENV_NAME+'_output.txt','a') as f:\n",
    "                    f.write(text+\"\\n\")\n",
    "\n",
    "                self.total_reward = 0\n",
    "                self.total_q_max = 0\n",
    "                self.total_loss = 0\n",
    "                self.duration = 0\n",
    "                self.episode += 1\n",
    "\n",
    "                total_episode += 1\n",
    "                if total_episode >= NUM_EPISODES:\n",
    "                    self.f_end = True\n",
    "\n",
    "            print(\"Actor\",self.num+1,\"is Over.\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "#以下メイン\n",
    "total_episode = 0\n",
    "remote_memory = Memory()\n",
    "\n",
    "# Train Mode\n",
    "if TRAIN:\n",
    "    if LOAD_NETWORK:\n",
    "        print('Train:ON \\nLoad_Network:ON')\n",
    "    else:\n",
    "        print('Train:On \\nLoad_Network:OFF')\n",
    "    sess = tf.InteractiveSession()\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        threads = [Learner(sess)] \n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        for i in range(NUM_ACTORS):\n",
    "            threads.append(Actor(number=i,sess=sess))\n",
    "\n",
    "    jobs = []\n",
    "    for worker in threads:\n",
    "        job = lambda: worker.run()\n",
    "        t = threading.Thread(target=job)\n",
    "        jobs.append(t)\n",
    "        t.start()\n",
    "\n",
    "\n",
    "    for t in jobs:\n",
    "        t.join()\n",
    "\n",
    "# Test Mode\n",
    "else:\n",
    "    if LOAD_NETWORK:\n",
    "        print('Train:OFF \\nLoad_Network:ON')\n",
    "    else:\n",
    "        print('Train:OFF \\nLoad_Network:OFF')\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env = wrappers.Monitor(env, SAVE_NETWORK_PATH, force=True)\n",
    "    sess = tf.InteractiveSession()\n",
    "    leaner = Learner(sess)\n",
    "    agent = Actor(number=i,sess=sess)\n",
    "    leaner.load_network()\n",
    "    agent.sess.run(agent.obtain_q_parameters)\n",
    "    for _ in range(NUM_EPISODES_AT_TEST):\n",
    "        terminal = False\n",
    "        observation = env.reset()\n",
    "        for _ in range(random.randint(1, NO_OP_STEPS)):\n",
    "            last_observation = observation\n",
    "            observation, _, _, _ = env.step(0)  # Do nothing\n",
    "        state = agent.get_initial_state(observation, last_observation)\n",
    "        while not terminal:\n",
    "            last_observation = observation\n",
    "            action = agent.get_action_at_test(state)\n",
    "            observation, _, terminal, _ = env.step(action)\n",
    "            env.render()\n",
    "            processed_observation = agent.preprocess(observation, last_observation)\n",
    "            state =np.append(state[1:, :, :], processed_observation, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
