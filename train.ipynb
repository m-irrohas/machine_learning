{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set params\n",
      "set data\n",
      "abababa\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "object float64 float64\n",
      "convert to Numpy (float32)\n",
      "<class 'list'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "float32 float32\n",
      "2176\n",
      "train dataset length: 2176\n",
      "test dataset length: 545\n",
      "set model\n",
      "run\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-9a38e088ac07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;31m## execution -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-9a38e088ac07>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_COL_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_COL_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "\"\"\"train.py\n",
    "\n",
    "This module trains LSTM text generator model.\n",
    "\n",
    "Quated from: https://spjai.com/ai-history/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## paths ---------------------------------------------------------------------\n",
    "import os\n",
    "import sys\n",
    "\n",
    "MAIN_PATH = os.path.dirname(os.path.abspath(__name__))\n",
    "MODELS_PATH = os.path.normpath(os.path.join(MAIN_PATH, 'models'))\n",
    "\n",
    "sys.path.append(MAIN_PATH)\n",
    "\n",
    "\n",
    "## modules ---------------------------------------------------------------------\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import Chain, Variable, datasets, optimizers\n",
    "from chainer import report, training\n",
    "from chainer.training import extensions\n",
    "import chainer.cuda\n",
    "\n",
    "from lstm import LSTMNet\n",
    "from lstm_updater import LSTMUpdater\n",
    "#from make_dataset import make_dataset\n",
    "\n",
    "\n",
    "## parameters ------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "## functions -------------------------------------------------------------------\n",
    "def main():\n",
    "    # params\n",
    "    print('set params')\n",
    "    n_lstm_layers = 1\n",
    "    n_in = 200\n",
    "    n_out = 200\n",
    "    n_hidden = 200\n",
    "    \n",
    "    dev_ratio = 0.2\n",
    "    batch_size = 16\n",
    "    epochs = 100\n",
    "    \n",
    "    gpu = -1\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    if gpu >= 0:\n",
    "        xp = cp\n",
    "    else:\n",
    "        xp = np\n",
    "\n",
    "        \n",
    "    # データの用意\n",
    "    print('set data')\n",
    "    print('abababa')\n",
    "    x_train, x_test, y_train, y_test = mkdataset(xp, dev_ratio)\n",
    "    print(len(y_train))\n",
    "    n_train = len(x_train)\n",
    "    n_test = len(x_test)\n",
    "    print('train dataset length: {}'.format(n_train))\n",
    "    print('test dataset length: {}'.format(n_test))    \n",
    "    \n",
    "    # モデルの宣言\n",
    "    print('set model')\n",
    "    rnn = LSTMNet(n_in, n_hidden, n_out)\n",
    "    model = L.Classifier(rnn, lossfun=F.mean_squared_error)\n",
    "\n",
    "    # GPU対応\n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(0).use()\n",
    "        model.to_gpu()\n",
    "        print('use GPU')\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "    \n",
    "    \n",
    "    early_stopping = 20\n",
    "    min_valid_loss = 1e-8\n",
    "    min_epoch = 0\n",
    "\n",
    "    train_loss, valid_loss = [], []\n",
    "    print('run')\n",
    "    for epoch in range(1, epochs):\n",
    "        #print('epch:{} is started'.format(epoch))\n",
    "        #_y = model(x=x_test, y=y_test)\n",
    "        #y_pred = _y.data\n",
    "        #y_pred = xp.array([1 - y, y], dtype='f').T[0]\n",
    "        #print(type(y_pred), y_pred.shape)\n",
    "        #y_t = xp.array(y_test)\n",
    "        #print(y_t.shape)\n",
    "        #accuracy = F.accuracy(y_pred, y_t).data\n",
    "\n",
    "        #_train_loss = F.sigmoid_cross_entropy(model(x_train), y_train).data\n",
    "        #_valid_loss = F.sigmoid_cross_entropy(_y, y_test).data\n",
    "        #train_loss.append(_train_loss)\n",
    "        #valid_loss.append(_valid_loss)\n",
    "\n",
    "        # valid_lossが20回連続で更新されなかった時点で学習を終了\n",
    "        #if min_valid_loss >= _valid_loss:\n",
    "            #min_valid_loss = _valid_loss\n",
    "            #min_epoch = epoch\n",
    "        #elif epoch - min_epoch >= early_stopping:\n",
    "            #break\n",
    "            \n",
    "        \"\"\"以下ogino\"\"\"\n",
    "        x,t = [], []\n",
    "        BATCH_ROW_SIZE=100\n",
    "        BATCH_COL_SIZE=100\n",
    "        #for i in range(0, n_train, batch_size):\n",
    "            #x_mini = x_train[i: i+batch_size]\n",
    "            #y_mini = y_train[i: i+batch_size]\n",
    "        for i in range(BATCH_ROW_SIZE):\n",
    "            index = np.random.randint(0,n_train-BATCH_COL_SIZE+1)\n",
    "            x.append(x_train[index:index+BATCH_COL_SIZE])\n",
    "            t.append(y_train[index:index+BATCH_COL_SIZE])\n",
    "        x = np.array(x, dtype='float32')\n",
    "        t = np.array(y, dtype='float32')\n",
    "        loss = 0\n",
    "        total_loss = 0\n",
    "        modell.reset()\n",
    "        \n",
    "        for i in range(BATCH_COL_SIZE):\n",
    "            x_ = np.array([x[j,i] for j in range(BATCH_ROW_SIZE)],dtype='float32')[:, np.newaxis]\n",
    "            t_ = np.array([t[j,i] for j in range(BATCH_ROW_SIZE)],dtype='float32')[:, np.newaxis]\n",
    "            loss += model(x=x_, t=t_, train=True)\n",
    "            optimizer.update()\n",
    "        \n",
    "            \n",
    "            \n",
    "            loss, accuracy = model(x=x_mini, y=y_mini, train=True)\n",
    "            loss.backward()\n",
    "\n",
    "            total_loss += loss.data\n",
    "            total_accuracy += accuracy.data\n",
    "            optimizer.update()\n",
    "        print('epoch: {} acc: {} loss: {} valid_loss: {}'.format(\n",
    "                    epoch, accuracy, _train_loss, _valid_loss))\n",
    "\n",
    "    #loss_plot(train_loss, valid_loss)\n",
    "    #serializers.save_npz('model.npz', model)\n",
    "\n",
    "\n",
    "def mkdataset(xp, dev_ratio=0.2):\n",
    "    # load data\n",
    "    MAIN_PATH = pathlib.Path(__name__).resolve().parent\n",
    "    x_file = MAIN_PATH / 'data_shaped' / 'x.npy'\n",
    "    t_file = MAIN_PATH / 'data_shaped' / 't.npy'\n",
    "    x = np.load(x_file)\n",
    "    t = np.load(t_file)\n",
    "    print(type(x), type(x[1]), type(x[1][1]))\n",
    "    print(x.dtype, x[1].dtype, x[1][1].dtype)\n",
    "    n_data = len(x)\n",
    "    n_train = int(len(x) * (1-dev_ratio))\n",
    "\n",
    "    # shuffle data\n",
    "    idx = np.arange(n_data)\n",
    "    np.random.shuffle(idx)\n",
    "    x = list(x[idx])\n",
    "    t = list(t[idx])\n",
    "\n",
    "    # convert data\n",
    "    x_raw = []\n",
    "    t_raw = []\n",
    "    if xp == cp:\n",
    "        print('convert to Cupy (float32)')\n",
    "        for x_sent, t_sent in zip(x, t):\n",
    "            x_raw.append(cp.asarray(x_sent, dtype=cp.float32))\n",
    "            t_raw.append(cp.asarray(t_sent, dtype=cp.float32))\n",
    "    else:\n",
    "        print('convert to Numpy (float32)')\n",
    "        for x_sent, t_sent in zip(x, t):\n",
    "            x_raw.append(x_sent.astype('float32'))\n",
    "            t_raw.append(t_sent.astype('float32'))\n",
    "    print(type(x_raw), type(x_raw[1]), type(x_raw[1][1]))\n",
    "    print(x_raw[1].dtype, x_raw[1][1].dtype)\n",
    "    \n",
    "    # split data\n",
    "    x_train = x_raw[:n_train]\n",
    "    y_train = t_raw[:n_train]\n",
    "    x_test = x_raw[n_train:]\n",
    "    y_test = t_raw[n_train:]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def loss_plot(train_loss, valid_loss):\n",
    "    import matplotlib.pyplot as plt\n",
    "    x = np.arange(len(train_loss))\n",
    "    plt.plot(x, train_loss)\n",
    "    plt.plot(x, valid_loss)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig('loss.png')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def abst_train():\n",
    "    # データの用意\n",
    "    print('set data')\n",
    "    MAIN_PATH = pathlib.Path(__name__).resolve().parent\n",
    "    x_file = MAIN_PATH / 'data_shaped' / 'x.npy'\n",
    "    t_file = MAIN_PATH / 'data_shaped' / 't.npy'\n",
    "    x = np.load(x_file)\n",
    "    t = np.load(t_file)\n",
    "    #x, t = make_dateset()\n",
    "    n_train = int(len(x) * (1-dev_ratio))\n",
    "    dataset = list(zip(x, t))\n",
    "    train, test = chainer.datasets.split_dataset(dataset, n_train)\n",
    "\n",
    "    # Iterator\n",
    "    print('set iterator')\n",
    "    train_iter = chainer.iterators.SerialIterator(train, batch_size)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, batch_size, repeat=False, shuffle=False)\n",
    "\n",
    "    # Updater &lt;- LSTM用にカスタマイズ\n",
    "    print('set updater')\n",
    "    updater = LSTMUpdater(train_iter, optimizer, device=gpu)\n",
    "\n",
    "    # Trainerとそのextensions\n",
    "    print('set trainer')\n",
    "    trainer = training.Trainer(updater, (epochs, 'epoch'), out='results')\n",
    "\n",
    "    # 評価データで評価\n",
    "    print('eval')\n",
    "    trainer.extend(extensions.Evaluator(test_iter, model, device=gpu))\n",
    "\n",
    "    # 学習結果の途中を表示する\n",
    "    print('log')\n",
    "    trainer.extend(extensions.LogReport(trigger=(1, 'epoch')))\n",
    "\n",
    "    # １エポックごとに、trainデータに対するlossと、testデータに対するlossを出力させる\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'validation/main/loss', 'elapsed_time']), trigger=(1, 'epoch'))\n",
    "\n",
    "    # 学習開始\n",
    "    print('run')\n",
    "    trainer.run()\n",
    "\n",
    "\n",
    "## execution -------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
