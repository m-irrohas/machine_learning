{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "#Requirement\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import MeCab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence\n",
    "\n",
    "#######################################################################\n",
    "#PATH\n",
    "PROJECT_PATH = \"../../../../project/question_generator\"\n",
    "DATA_PATH=PROJECT_PATH + \"/data/takken/\"\n",
    "MODEL_PATH=\"./saved_model/\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "latent_dim = 512  # Latent dimensionality of the encoding space.\n",
    "\n",
    "########################################################################\n",
    "##class\n",
    "#整形クラス\n",
    "class Vocab(object):\n",
    "    '''単語とIDのペアを管理するクラス。\n",
    "    Attributes:\n",
    "        min_count: 未実装，min_count以下の出現回数の単語はVocabに追加しないようにする\n",
    "        \n",
    "    TODO:\n",
    "        add min_count option\n",
    "    '''\n",
    "\n",
    "    def __init__(self, min_count=0):\n",
    "        self.word2id_dict = dict({'<PAD>': 0, '<UNK>': 1})\n",
    "        self.id2word_dict = dict(\n",
    "            {i: word\n",
    "             for word, i in self.word2id_dict.items()})\n",
    "        self.size = 2\n",
    "        self.min_count = min_count\n",
    "        self._i = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._i == self.size:\n",
    "            self._i = 0\n",
    "            raise StopIteration\n",
    "        word = self.id2word(self._i)\n",
    "        self._i += 1\n",
    "        return word\n",
    "\n",
    "    def add(self, word):\n",
    "        '''\n",
    "        Args:\n",
    "            word(string):単語\n",
    "        if word is not in Vocab, then add it\n",
    "        '''\n",
    "        key = self.word2id_dict.setdefault(word, self.size)\n",
    "        self.id2word_dict[key] = word\n",
    "        if key == self.size:\n",
    "            self.size += 1\n",
    "\n",
    "    def word2id(self, word):\n",
    "        '''\n",
    "        Args:\n",
    "            word(string):単語\n",
    "        Returns:\n",
    "            returns id allocated to word if it's in Vocab. Otherwise, returns 1 which means unknown word.\n",
    "        '''\n",
    "        return self.word2id_dict.get(word, 1)  #1 means <UNK>\n",
    "\n",
    "    def id2word(self, key):\n",
    "        '''\n",
    "        Args:\n",
    "            key(int)\n",
    "        Returns:\n",
    "            returns word allocated to key if it's in Vocab. Otherwise, returns <UNK>.\n",
    "        '''\n",
    "        return self.id2word_dict.get(key, '<UNK>')\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        '''update vocab\n",
    "        Args:\n",
    "            sentences:list of lists,each element of list is one sentence,\n",
    "            each sentence is represented as list of words\n",
    "        '''\n",
    "        assert isinstance(sentences, list)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            assert isinstance(sentence, list)\n",
    "            for word in sentence:\n",
    "                self.add(word)\n",
    "\n",
    "    def seq2ids(self, sentence):\n",
    "        '''\n",
    "        Args:\n",
    "            sequence: list each element of which is word(string)\n",
    "        Returns:\n",
    "            list each element of which is id(int) corresponding to each word\n",
    "        '''\n",
    "        assert isinstance(sentence, list)\n",
    "        id_seq = list()\n",
    "        for word in sentence:\n",
    "            id_seq.append(self.word2id(word))\n",
    "\n",
    "        return id_seq\n",
    "\n",
    "    def ids2seq(self, id_seq):\n",
    "        '''inverse processing of seq2ids\n",
    "        '''\n",
    "        assert isinstance(id_seq, list)\n",
    "        sentence = list()\n",
    "        for key in id_seq:\n",
    "            sentence.append(self.id2word(key))\n",
    "        return sentence\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    '''Data loader to return minibatches of input sequence and target sequence an iteration\n",
    "    Attributes:\n",
    "        input_seq: input sequence, numpy ndarray\n",
    "        target_seq: target sequence, numpy ndarray\n",
    "        input_lengths: true lengths of input sequences, before padding\n",
    "        batch_size: batch size\n",
    "    '''\n",
    "\n",
    "    def __init__(self, src_seq, tgt_seq, src_lengths, batch_size):\n",
    "        self.src_seq = src_seq\n",
    "        self.tgt_seq = tgt_seq\n",
    "        self.src_lengths = src_lengths\n",
    "        self.batch_size = batch_size\n",
    "        self.size = len(self.src_seq)\n",
    "        self.start_index = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''shuffle data'''\n",
    "        self.src_seq, self.tgt_seq, self.src_lengths = shuffle(\n",
    "            self.src_seq, self.tgt_seq, self.src_lengths)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_index >= self.size:\n",
    "            self.reset()\n",
    "            self.start_index = 0\n",
    "            raise StopIteration\n",
    "        batch_X = self.src_seq[self.start_index:self.start_index +\n",
    "                                 self.batch_size]\n",
    "        batch_Y = self.tgt_seq[self.start_index:self.start_index +\n",
    "                                  self.batch_size]\n",
    "        lengths = self.src_lengths[self.start_index:self.start_index +\n",
    "                                     self.batch_size]\n",
    "        self.start_index += self.batch_size\n",
    "\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.int32, device=device)\n",
    "\n",
    "        return batch_X, batch_Y, lengths\n",
    "\n",
    "###############################################################################\n",
    "#Encoder Decorder Attentionクラス\n",
    "#To DO: Transformer の実装\n",
    "class BiEncoder(nn.Module):\n",
    "    '''Bidirectional Encoder\n",
    "    Attributes:\n",
    "        num_vocab: vocabulary size of input sequences\n",
    "        embedding_dim: dimensions of embedding vector\n",
    "        hidden_size: hidden dimensions of LSTM\n",
    "        embedding_matrix: initial values of embedding matrix\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_vocab, embedding_dim, hidden_size,\n",
    "                 embedding_matrix):\n",
    "        super(BiEncoder, self).__init__()\n",
    "        embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(\n",
    "            num_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0,\n",
    "            _weight=embedding_matrix)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=True)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        '''\n",
    "        Args:\n",
    "            x: input sequence (batch_size, seq_len)\n",
    "            lengths: tensor that retains true lengths before padding\n",
    "        Returns:\n",
    "            output: LSTM output\n",
    "            (h, c): LSTM states at last timestep\n",
    "        '''\n",
    "        embed = self.embed(x).permute(1, 0, 2).to(\n",
    "            device)  #(seq_len, batch_size, embedding_dim)\n",
    "\n",
    "        embed = pack_padded_sequence(\n",
    "            embed, lengths=lengths\n",
    "        )  #(any_len, batch_size, embedding_dim),もう少し調べてから実装する\n",
    "\n",
    "        output, (h, c) = self.bilstm(\n",
    "            embed\n",
    "        )  #(any_len, batch_size, 2*hidden_size), (2, batch_size, hidden_size)\n",
    "        # reshape states into (1,batch_size, 2*hidden_size)\n",
    "        h = h.permute(1, 2, 0).contiguous().view(1, -1, 2 * self.hidden_size)\n",
    "        c = c.permute(1, 2, 0).contiguous().view(1, -1, 2 * self.hidden_size)\n",
    "\n",
    "        output = pad_packed_sequence(\n",
    "            output)  #(seq_len, batch_size, hidden_size)\n",
    "\n",
    "        return output, (h, c)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):   \n",
    "    '''NN decoding from encoder's last states\n",
    "    Args:\n",
    "        num_vocab: vocabulary size of target sequences\n",
    "        embedding_dim: dimensions of embedding vector\n",
    "        hidden_size: hidden dimensions of LSTM\n",
    "        embedding_matrix: initial values of embedding matrix\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_vocab, embedding_dim, hidden_size,\n",
    "                 embedding_matrix):\n",
    "        super(Decoder, self).__init__()\n",
    "        embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(\n",
    "            num_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0,\n",
    "            _weight=embedding_matrix)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, num_vocab)\n",
    "\n",
    "    def forward(self, decoder_input, decoder_states):\n",
    "        '''\n",
    "        Args:\n",
    "            decoder_input: tensor (batch_size, seq_len)\n",
    "            decoder_states: LSTM's initial state (1, batch_size, hidden_dim)\n",
    "            \n",
    "        Returns:\n",
    "            output: LSTM output shape=(seq_len,batch_size,num_vocab)\n",
    "            hidden: tuple of last states, both shape=(1,batch_size,hidden_dim)\n",
    "        '''\n",
    "        embed = self.embed(decoder_input)  #(batch_size,seq_len,embedding_dim)\n",
    "        assert len(embed.size()) == 3, '{}'.format(embed.size())\n",
    "        output, hidden = self.lstm(\n",
    "            embed.permute(1, 0, 2), decoder_states\n",
    "        )  #(seq_len,batch_size,hidden_dim),(1,batch_size,hidden_dim)\n",
    "        output = self.linear(output)  #(seq_len,batch_size,num_vocab)\n",
    "\n",
    "        return output, hidden  # (seq_len,batch_size,num_vocab), tuple of (1,batch_size,hidden_dim)\n",
    "\n",
    "\n",
    "class GlobalAttentionDecoder(nn.Module):\n",
    "    '''Decoder using Global Attention mechanism\n",
    "     Args:\n",
    "        num_vocab: vocabulary size of target sequences\n",
    "        embedding_dim: dimensions of embedding vector\n",
    "        hidden_size: hidden dimensions of LSTM\n",
    "        maxlen: maximum length of input sequences\n",
    "        embedding_matrix: initial values of embedding matrix\n",
    "        dropout_p: probability of dropout occurrence, Default:0.2\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_vocab,\n",
    "                 embedding_dim,\n",
    "                 hidden_size,\n",
    "                 maxlen,\n",
    "                 embedding_matrix,\n",
    "                 dropout_p=0.2):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.maxlen = maxlen\n",
    "        self.dropout_p = dropout_p\n",
    "        embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "        self.embed = nn.Embedding(\n",
    "            num_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=0,\n",
    "            _weight=embedding_matrix)\n",
    "        #self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, hidden_size=self.hidden_size)\n",
    "        self.out = nn.Linear(2 * hidden_size, num_vocab)\n",
    "\n",
    "    def forward(self, decoder_input, hidden, encoder_outputs):\n",
    "        '''\n",
    "        Args:\n",
    "            decoder_input: (batch_size, seq_len),seq_len must be 1\n",
    "            hidden: LSTM initial state, tuple(h,c)\n",
    "            encoder_outputs: (seq_len,batch_size,hidden_size)\n",
    "        Returns:\n",
    "            output: LSTM output\n",
    "            hidden: LSTM last states\n",
    "            attn_weights: attention score of each timesteps\n",
    "        '''\n",
    "        seq_len = decoder_input.size(1)\n",
    "        assert seq_len == 1\n",
    "        embed = self.embedding(\n",
    "            decoder_input\n",
    "        )  # (batch_size, seq_len, embedding_dim), and seq_len must be 1\n",
    "        embed = embed.permute(1, 0, 2)  # (seq_len,batch_size,embedding_dim)\n",
    "        #embed = self.dropout(embed)\n",
    "\n",
    "        output, (h, c) = self.lstm(\n",
    "            embed, hidden\n",
    "        )  #(seq_len,batch_size,hidden_size),tuple of (1,batch_size,hidden_size)\n",
    "\n",
    "        attn_scores = encoder_outputs.permute(1, 0, 2).bmm(h.permute(\n",
    "            1, 2, 0))  #(batch_size,seq_len,1)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores, dim=1)  #(batch_size,seq_len,1)\n",
    "        context = encoder_outputs.permute(1, 2, 0).bmm(attn_weights).squeeze(\n",
    "            2)  #(batch_size,hidden_size)\n",
    "        output = torch.cat(\n",
    "            [context, h.squeeze(0)], dim=1)  #(batch_size,2*hidden_size)\n",
    "        output = self.out(output)  #(batch_size, num_vocab)\n",
    "        return output, hidden, attn_weights  #attn_weights will be used to visualize how attention works\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, src_num_vocab, tgt_num_vocab, embedding_dim,\n",
    "                 hidden_size, src_embedding_matrix,\n",
    "                 tgt_embedding_matrix):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = BiEncoder(src_num_vocab, embedding_dim, hidden_size,\n",
    "                                 src_embedding_matrix)\n",
    "        self.decoder = Decoder(tgt_num_vocab, embedding_dim,\n",
    "                               2 * hidden_size, tgt_embedding_matrix)\n",
    "\n",
    "    def forward(self, src, tgt, lengths, dec_vocab, is_teacher_forcing):\n",
    "        output, encoder_states = self.encoder(src, lengths)\n",
    "\n",
    "        tgt_length = tgt.size(1)  #tgt.shape(batch_size, seq_len)\n",
    "        batch_size = tgt.size(0)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        decoder_states = encoder_states\n",
    "        decoder_input = torch.tensor(\n",
    "            [dec_vocab.word2id(\"<BOS>\")] * batch_size,\n",
    "            dtype=torch.long,\n",
    "            device=device).unsqueeze(1)  #(batch_size,1)\n",
    "\n",
    "        for i in range(tgt_length):\n",
    "            output, decoder_states = self.decoder(\n",
    "                decoder_input, decoder_states) # (1,batch_size,vocab_size)\n",
    "            topv, topi = torch.max(output, 2) # (1, batch_size)\n",
    "            outputs.append(output)\n",
    "            if is_teacher_forcing:\n",
    "                decoder_input = tgt[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                #topi.detach()\n",
    "                decoder_input = topi.permute(1, 0)  #(batch_size, 1)\n",
    "\n",
    "        outputs = torch.cat(\n",
    "            outputs, dim=0).permute(1, 2, 0)  #(batch_size,vocab_size,seq_len)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GlobalAttentionEncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 src_num_vocab,\n",
    "                 tgt_num_vocab,\n",
    "                 embedding_dim,\n",
    "                 hidden_size,\n",
    "                 src_embedding_matrix,\n",
    "                 tgt_embedding_matrix,\n",
    "                 dropout_p=0.2):\n",
    "        super(GlobalAttentionEncoderDecoder, self).__init__()\n",
    "        self.encoder = BiEncoder(src_num_vocab, embedding_dim, hidden_size,\n",
    "                                 src_embedding_matrix)\n",
    "        self.decoder = GlobalAttentionDecoder(\n",
    "            tgt_num_vocab,\n",
    "            embedding_dim,\n",
    "            2 * hidden_size,\n",
    "            tgt_embedding_matrix,\n",
    "            dropout_p=dropout_p)\n",
    "\n",
    "    def forward(self, src, tgt, lengths, dec_vocab, is_teacher_forcing):\n",
    "        pass\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#文章整形クラス\n",
    "def remove_choice_number(text):\n",
    "    '''文頭に選択肢番号がついている場合それを除く。\n",
    "    前処理で使うだけなのでこのファイルでは呼び出さない。別のファイルに移したい。\n",
    "    '''\n",
    "    remove_list = [\n",
    "        \"^ア \", \"^イ \", \"^ウ \", \"^エ \", \"^オ \", \"^1 \", \"^2 \", \"^3 \", \"^4 \", \"^5 \"\n",
    "    ]\n",
    "    for i, word in enumerate(remove_list):\n",
    "        text = re.sub(word, \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_symbol(text):\n",
    "    '''\n",
    "    入力されたテキストから句読点などの不要な記号をいくつか削除する。\n",
    "    '''\n",
    "    remove_list = [\n",
    "        ',', '.', '-', '、', '，', '。', '\\ufeff', '\\u3000', '「', '」', '（', '）',\n",
    "        '(', ')','\\n'\n",
    "    ]\n",
    "    for i, symbol in enumerate(remove_list):\n",
    "        text = text.replace(symbol, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_bos_eos(text):\n",
    "    '''\n",
    "    文章の先頭に<BOS>、<EOS>を加える。文末の改行コードの都合で<EOS>の直前にはスペースを入れていない。\n",
    "    '''\n",
    "    return \"<BOS> \" + text + \"<EOS>\"\n",
    "\n",
    "\n",
    "def replace_number(text):\n",
    "    '''textの数値表現を<Number>トークンに置き換える\n",
    "    textは分かち書きされていること\n",
    "    '''\n",
    "    new_text = \"\"\n",
    "    for word in text.split(' '):\n",
    "        if word.isnumeric():\n",
    "            new_text += \"<NUM> \"\n",
    "        elif word == \"<EOS>\":\n",
    "            new_text += \"<EOS>\"\n",
    "        else:\n",
    "            new_text += word + \" \"\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def isalpha(s):\n",
    "    '''\n",
    "    Args:\n",
    "        s:string\n",
    "    Returns:\n",
    "        bool:sが半角英字から成るかどうか\n",
    "    '''\n",
    "    alphaReg = re.compile(r'^[a-zA-Z]+$')\n",
    "    return alphaReg.match(s) is not None\n",
    "\n",
    "\n",
    "def replace_alphabet(text):\n",
    "    '''\n",
    "    Args:\n",
    "    text:分かち書きされた文。\n",
    "    Return:\n",
    "    textの数値表現をAに置き換える\n",
    "    '''\n",
    "    new_text = \"\"\n",
    "    for word in text.split(' '):\n",
    "        if isalpha(word):\n",
    "            new_text += \"A \"\n",
    "        elif word == \"<EOS>\":\n",
    "            new_text += word\n",
    "        else:\n",
    "            new_text += word + \" \"\n",
    "    return new_text\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "#Trainクラス\n",
    "def train(src,\n",
    "          tgt,\n",
    "          lengths,\n",
    "          model,\n",
    "          optimizer,\n",
    "          criterion,\n",
    "          dec_vocab,\n",
    "          is_train=True,\n",
    "          teacher_forcing_ratio=0.8):\n",
    "    '''一回のミニバッチ学習\n",
    "    Args:\n",
    "        src:入力文\n",
    "        tgt:正解文\n",
    "        model:EncoderDecoderモデル\n",
    "        optimizer:torch.optim\n",
    "        criterion:損失関数\n",
    "        decoder_vocab:Decoder側のVocabクラス\n",
    "        is_train:bool\n",
    "        teacher_forcing_ration:teacher forcingを実行する確率\n",
    "    Returns:\n",
    "        loss: averaged loss of all tokens\n",
    "    '''\n",
    "\n",
    "    #minibatch学習のたびにteacher forcingするかどうかをサンプルしている、単語ごとにサンプルするのがScheduled Sampling\n",
    "    is_teacher_forcing = True if np.random.random(\n",
    "    ) < teacher_forcing_ratio else False\n",
    "\n",
    "    y_pred = model(src, tgt, lengths, dec_vocab, is_teacher_forcing)\n",
    "\n",
    "    loss = criterion(y_pred, tgt)\n",
    "\n",
    "    if is_train:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def trainIters(model,\n",
    "               criterion,\n",
    "               train_dataloader,\n",
    "               valid_dataloader,\n",
    "               decoder_vocab,\n",
    "               epochs=epochs,\n",
    "               batch_size=batch_size,\n",
    "               print_every=1,\n",
    "               plot_every=5,\n",
    "               teacher_forcing_ratio=0.8):\n",
    "    '''Encoder-Decoderモデルの学習\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #validation dataがないのでしっかりそれも書く\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    plot_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        for batch_id, (batch_X, batch_Y,\n",
    "                       X_lengths) in enumerate(train_dataloader):\n",
    "            loss = train(\n",
    "                batch_X,\n",
    "                batch_Y,\n",
    "                X_lengths,\n",
    "                model,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                decoder_vocab,\n",
    "                is_train=True,\n",
    "                teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            train_loss += loss\n",
    "            if batch_id % print_every == 0:\n",
    "                elapsed_sec = time.time() - start\n",
    "                elapsed_min = int(elapsed_sec / 60)\n",
    "                elapsed_sec = elapsed_sec - 60 * elapsed_min\n",
    "                print(\n",
    "                    'Epoch:{} Batch:{}/{} Loss:{:.4f} Time:{}m{:.1f}s'.format(\n",
    "                        epoch, batch_id,\n",
    "                        int(train_dataloader.size /\n",
    "                            train_dataloader.batch_size),\n",
    "                        train_loss / (1 + batch_id), elapsed_min,\n",
    "                        elapsed_sec),\n",
    "                    end='\\r')\n",
    "        print()\n",
    "\n",
    "        for batch_id, (batch_X, batch_Y,\n",
    "                       X_lengths) in enumerate(valid_dataloader):\n",
    "            loss = train(\n",
    "                batch_X,\n",
    "                batch_Y,\n",
    "                X_lengths,\n",
    "                model,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                decoder_vocab,\n",
    "                is_train=False,\n",
    "                teacher_forcing_ratio=0)\n",
    "            valid_loss += loss\n",
    "            if batch_id % plot_every:\n",
    "                plot_losses.append(loss)\n",
    "\n",
    "        mean_valid_loss = valid_loss / (1 + batch_id)\n",
    "        print('Epoch:{} Valid Loss:{:.4f}'.format(epoch, mean_valid_loss))\n",
    "\n",
    "    return plot_losses\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "#過去12年分の宅建の過去問\n",
    "takken = pd.read_csv(DATA_PATH+\"takken.csv\", encoding='utf-8')\n",
    "mondaishu = pd.read_csv(DATA_PATH+\"mondaishu.csv\", encoding='utf-8')\n",
    "nikken = pd.read_csv(DATA_PATH+\"nikken.csv\",encoding='utf-8')\n",
    "legal_mind = pd.read_csv(DATA_PATH+\"legal_mind.csv\",encoding='utf-8')\n",
    "\n",
    "#データをまとめる\n",
    "takken = takken[[\"Question\", \"Choice\"]]\n",
    "ocr = pd.concat([mondaishu,nikken,legal_mind],axis=0,ignore_index=True)\n",
    "ocr = ocr[[\"Wakati_Question\",\"Wakati_Choice\"]]\n",
    "ocr.columns = [\"Question\",\"Choice\"]\n",
    "\n",
    "#データをMeCabで分かち書きして、不要な文字の除去、BOS,EOSの追加を行う\n",
    "m = MeCab.Tagger(\"-Owakati\")\n",
    "takken = takken.applymap(remove_symbol)\n",
    "ocr = ocr.applymap(remove_symbol)\n",
    "takken = takken.applymap(m.parse)\n",
    "takken = pd.concat([takken,ocr],axis=0,ignore_index=True)\n",
    "takken = takken.applymap(remove_symbol)\n",
    "takken = takken.applymap(add_bos_eos)\n",
    "takken = takken.applymap(replace_number)\n",
    "takken = takken.applymap(replace_alphabet)\n",
    "print(\"data size is\",len(takken))\n",
    "#takken.head()\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "##長い系列は推論が難しいので一定程度長い文はデータから取り除く\n",
    "q_maxlen = 150\n",
    "c_maxlen = 110\n",
    "maxlen = max(q_maxlen, c_maxlen)\n",
    "\n",
    "takken = takken[takken[\"Question\"].str.split(' ').apply(len) <= q_maxlen]\n",
    "takken = takken[takken[\"Choice\"].str.split(' ').apply(len) <= c_maxlen]\n",
    "takken.reset_index(drop=True, inplace=True)\n",
    "input_lengths = takken.apply(len,axis=1)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "#make dictionary\n",
    "c_words = Vocab()\n",
    "q_words = Vocab()\n",
    "c_words.add(\"<NUM>\")\n",
    "q_words.add(\"<NUM>\")\n",
    "for i in range(len(takken)):\n",
    "    for word in (takken.loc[i, \"Question\"]).split():\n",
    "        q_words.add(word)\n",
    "    for word in (takken.loc[i, \"Choice\"]).split():\n",
    "        c_words.add(word)\n",
    "        \n",
    "with open('choice.vocab','wb') as f:\n",
    "    pickle.dump(c_words,f)\n",
    "\n",
    "with open('question.vocab','wb') as f:\n",
    "    pickle.dump(q_words,f)\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "num_encoder_tokens = c_words.size\n",
    "num_decoder_tokens = q_words.size\n",
    "print(\"vocabulary size in choices is\", num_encoder_tokens)\n",
    "print(\"vocabulary size in questions is\", num_decoder_tokens)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "#Embedding層の初期値としてpre-trainさせたword2vec embeddingを用いる。\n",
    "#単語辞書の中にはword2vecモデルに含まれない単語もあるので、そのembeddingは一様乱数で初期化する\n",
    "word2vec = Word2Vec.load(PROJECT_PATH + \"/qa_qg/wiki_textbook/text_wiki_model\")\n",
    "\n",
    "word2vec_size = 200\n",
    "encoder_embedding_matrix = np.random.uniform(\n",
    "    low=-0.05, high=0.05, size=(num_encoder_tokens, word2vec_size))\n",
    "decoder_embedding_matrix = np.random.uniform(\n",
    "    low=-0.05, high=0.05, size=(num_decoder_tokens, word2vec_size))\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "unknown_set = set()\n",
    "\n",
    "for i, word in enumerate(c_words):\n",
    "    try:\n",
    "        encoder_embedding_matrix[i] = word2vec[word]\n",
    "    except KeyError:\n",
    "        if word not in unknown_set:\n",
    "            unknown_set.add(word)\n",
    "for i, word in enumerate(q_words):\n",
    "    try:\n",
    "        decoder_embedding_matrix[i] = word2vec[word]\n",
    "    except KeyError:\n",
    "        if word not in unknown_set:\n",
    "            unknown_set.add(word)\n",
    "            \n",
    "encoder_embedding_matrix[0]=np.zeros((word2vec_size,))\n",
    "decoder_embedding_matrix[0]=np.zeros((word2vec_size,))\n",
    "\n",
    "encoder_embedding_matrix = encoder_embedding_matrix.astype('float32')\n",
    "decoder_embedding_matrix = decoder_embedding_matrix.astype('float32')\n",
    "\n",
    "unknown_set.remove(\"<NUM>\")\n",
    "unknown_set.remove(\"<UNK>\")\n",
    "unknown_set.remove(\"<PAD>\")\n",
    "unknown_set.remove(\"<BOS>\")\n",
    "unknown_set.remove(\"<EOS>\")\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "unknown_set\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "#Vocab classに合わせてlistで管理したい\n",
    "datasize = takken.shape[0]\n",
    "choice = np.zeros((datasize, c_maxlen),dtype='int32')\n",
    "question = np.zeros((datasize, q_maxlen),dtype='int32')\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "for i in range(datasize):\n",
    "    for j, word in enumerate(takken.loc[i, \"Choice\"].split(' ')):\n",
    "        if word in unknown_set:\n",
    "            word = \"<UNK>\"\n",
    "        choice[i][j] = c_words.word2id(word)\n",
    "    for j, word in enumerate(takken.loc[i, \"Question\"].split(' ')):\n",
    "        if word in unknown_set:\n",
    "            word = \"<UNK>\"\n",
    "        question[i][j] = q_words.word2id(word)\n",
    "        \n",
    "choice = choice[:, :-1]\n",
    "question = question[:, 1:]\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) #not to include <PAD> in loss calculation\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "model = EncoderDecoder(c_words.size,q_words.size,200,latent_dim,encoder_embedding_matrix,decoder_embedding_matrix).to(device)\n",
    "\n",
    "input_lengths = np.array(input_lengths)\n",
    "\n",
    "train_choice, valid_choice, train_question, valid_question, train_input_lengths, valid_input_lengths = train_test_split(\n",
    "choice,question,input_lengths)\n",
    "\n",
    "train_dataloader = DataLoader(train_choice,train_question,train_input_lengths,batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(valid_choice,valid_question,valid_input_lengths,batch_size=batch_size)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "losses = trainIters(model,criterion,train_dataloader,valid_dataloader,q_words)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.savefig('loss.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
