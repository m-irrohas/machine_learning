{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データローダー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIREMENT\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "font = {\"family\":'ipag'}\n",
    "mpl.rc('font', **font)\n",
    "import MeCab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence\n",
    "from nltk import bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenDataLoader(object):\n",
    "    def __init__(self, file_path, batch_size):\n",
    "        super(GenDataLoader, self).__init__()\n",
    "        self.data = self.load_file(file_path)\n",
    "        self.batch_size = batch_size\n",
    "        self.pointer = 0\n",
    "        self.data_num = len(self.data)\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.pointer >= self.data_num:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "        batch = torch.tensor(self.data[self.pointer : self.pointer+self.batch_size], \n",
    "                             dtype=torch.long)\n",
    "        batch_X = batch[:, :-1].to(device)\n",
    "        batch_Y = batch[:, 1:].to(device)\n",
    "        self.pointer += self.batch_size\n",
    "        return batch_X, batch_Y\n",
    "    \n",
    "    def load_file(self, file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                line = [int(x) for x in line]\n",
    "                data.append(line)\n",
    "        return data\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pointer = 0\n",
    "        random.shuffle(self.data)\n",
    "        \n",
    "class DisDataLoader(object):\n",
    "    def __init__(self, positive_file, negative_file, batch_size):\n",
    "        super(DisDataLoader, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        pos_data = self.load_file(positive_file)\n",
    "        neg_data = self.load_file(negative_file)\n",
    "        self.data = pos_data + neg_data\n",
    "        \n",
    "        self.labels = [1 for _ in range(len(pos_data))] + [0 for _ in range(len(neg_data))]\n",
    "        self.pairs = list(zip(self.data, self.labels))\n",
    "        self.data_num = len(self.pairs)\n",
    "        self.pointer = 0\n",
    "        self.reset()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.pointer >= self.data_num:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "        \n",
    "        batch_X, batch_Y = zip(*self.pairs[self.pointer : self.pointer+self.batch_size])\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "        self.pointer += self.batch_size\n",
    "        return batch_X, batch_Y\n",
    "            \n",
    "    def load_file(self, file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                line = [int(x) for x in line]\n",
    "                data.append(line)\n",
    "        return data\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pointer = 0\n",
    "        random.shuffle(self.pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, batch_size, \n",
    "                 embedding_size, hidden_size, max_length):\n",
    "        super(Generator, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding= nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, num_layers=1,\n",
    "                           batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        \n",
    "    def forward(self, seqs):\n",
    "        N = seqs.size(0)\n",
    "        embed = self.embedding(seqs)\n",
    "        h, c = self.init_hidden(N)\n",
    "        self.lstm.flatten_parameters()\n",
    "        hidden, (h, c) =self.lstm(embed, (h, c))\n",
    "        lin =self.linear(hidden)\n",
    "        outputs = F.log_softmax(lin, dim=-1)\n",
    "        outputs =outputs.view(-1, self.vocab_size)\n",
    "        return outputs\n",
    "    \n",
    "    def step(self, x, h, c):\n",
    "        embed =self.embedding(x)\n",
    "        self.lstm.flatten_parameters()\n",
    "        hidden, (h, c) = self.lstm(embed, (h, c))\n",
    "        pred =F.softmax(self.linear(hidden), dim=-1)\n",
    "        return pred, h, c\n",
    "    \n",
    "    def sample(self, x=None):\n",
    "        flag =False\n",
    "        if x is None:\n",
    "            flag = True\n",
    "        if flag:\n",
    "            x = torch.empty(self.batch_size, 1).fill_(BOS).long().to(device)\n",
    "        h, c = self.init_hidden(self.batch_size)\n",
    "        \n",
    "        samples = []\n",
    "        if flag:\n",
    "            for i in range(self.max_length):\n",
    "                output, h, c = self.step(x, h, c)\n",
    "                output = output.squeeze(1)\n",
    "                x = output.multinomial(1)\n",
    "                samples.append(x)\n",
    "                \n",
    "        else:\n",
    "            given_len = x.size(1)\n",
    "            lis = x.chunk(x.size(1) ,dim=1)\n",
    "            for i in range(given_len):\n",
    "                output, h, c = self.step(lis[i], h, c)\n",
    "                samples.append(lis[i])\n",
    "            output = output.squeeze(1)\n",
    "            x = output.multinomial(1)\n",
    "            for i in range(given_len, self.max_length):\n",
    "                samples.append(x)\n",
    "                output, h, c = self.step(x, h, c)\n",
    "                output = output.squeeze(1)\n",
    "                x = output.multinomial(1)\n",
    "        output = torch.cat(samples, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self, N):\n",
    "        h0 = torch.zeros(1, N, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(1, N, self.hidden_size).to(device)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    def __init__(self, input_size, num_layers=1, f=F.relu):\n",
    "        super(Highway, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.linear1 = nn.ModuleList([nn.Linear(input_size, input_size) for _ in range(num_layers)])\n",
    "        self.linear2 = nn.ModuleList([nn.Linear(input_size, input_size) for _ in range(num_layers)])\n",
    "        self.f = f\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in range(self.num_layers):\n",
    "            gate = torch.sigmoid(self.linear1[layer](x))\n",
    "            nonlinear =self.f(self.linear2[layer](x))\n",
    "            x = gate * nonlinear + (1 - gate) * x\n",
    "        return x\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, batch_size, embedding_size,\n",
    "                num_filters, filter_sizes, dropout_prob=0.75):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        assert len(self.num_filters) ==len(self.filter_sizes)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, Co, (K, self.embedding_size))\n",
    "                                  for Co, K in zip(self.num_filters, self.filter_sizes)])\n",
    "        \n",
    "        self.highway = Highway(sum(self.num_filters))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.linear = nn.Linear(sum(self.num_filters), 2)\n",
    "        \n",
    "    def forward(self, x, log=True):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.highway(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        if log:\n",
    "            x = F.log_softmax(x, dim=-1)\n",
    "        else:\n",
    "            x = F.softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rollout():\n",
    "    def __init__(self, model, update_rate):\n",
    "        self.ori_model = model\n",
    "        self.own_model = copy.deepcopy(model)\n",
    "        self.update_rate = update_rate\n",
    "        \n",
    "    def get_reward(self, x, num, discriminator):\n",
    "        rewards = []\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        for i in range(num):\n",
    "            for t in range(1, seq_length):\n",
    "                data = x[:, 0:t]\n",
    "                samples = self.own_model.sample(x=data)\n",
    "                pred = discriminator(samples, log=False)\n",
    "                pred = pred.cpu().data[:, 1].numpy()\n",
    "                if i == 0:\n",
    "                    rewards.append(pred)\n",
    "                else:\n",
    "                    rewards[t-1] += pred\n",
    "            pred = discriminator(x, log=False)\n",
    "            pred = pred.cpu().data[:, 1].numpy()\n",
    "            if i == 0:\n",
    "                rewards.append(pred)\n",
    "            else:\n",
    "                rewards[seq_length-1] += pred\n",
    "            \n",
    "        rewards = np.array(rewards)\n",
    "        rewarda = np.transpose(rewards) / (1.0*num)\n",
    "        return rewards\n",
    "        \n",
    "    def update_params(self):\n",
    "        dic = {}\n",
    "        for name, param in self.own_model.named_parameters():\n",
    "            dic[name] = param.data\n",
    "        for name, param in self.ori_model.named_parameters():\n",
    "            if name.startswith('emb'):\n",
    "                param.data = dic[name]\n",
    "            else:\n",
    "                param.data = (1- self.update_rate) * param.data + self.update_rate * dic[name]\n",
    "        self.own_model = copy.deepcopy(self.ori_model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deviceはGPUを使用します\n",
      "data size is 9442\n",
      "['<BOS>', '建設', '業法', 'による', '建設', '業', 'の', '許可', 'を', '受け', 'て', 'いる', 'A', 'が', '建築', '請負', '契約', 'に', '付帯', 'し', 'て', '取り決め', 'た', '約束', 'を', '履行', 'する', 'ため', '建築', 'し', 'た', '共同', '住宅', 'の', '売買', 'の', 'あっせん', 'を', '反復', '継続', 'し', 'て', '行う', '場合', '<EOS>']\n",
      "語彙数\t: 4523\n"
     ]
    }
   ],
   "source": [
    "#PATH\n",
    "PROJECT_PATH = \"../../../../project/question_generator\"\n",
    "DATA_PATH=PROJECT_PATH + \"/data/takken/\"\n",
    "MODEL_PATH=\"./saved_model/\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#DEVICE\n",
    "if device == torch.device('cuda'):\n",
    "    print('deviceはGPUを使用します')\n",
    "else:\n",
    "    print('deviceはCPUを使用します')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#データのロード\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        words = line.strip().split()\n",
    "        data.append(words)\n",
    "    return data\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#Vocabクラス\n",
    "##wordからidを生成\n",
    "##idからwordを生成\n",
    "class Vocab(object):\n",
    "    def __init__(self, word2id={}):\n",
    "        \"\"\"\n",
    "        word2id: 単語(str)をインデックス(int)に変換する辞書\n",
    "        id2word: インデックス(int)を単語(str)に変換する辞書\n",
    "        \"\"\"\n",
    "        self.word2id = dict(word2id)\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}    \n",
    "        \n",
    "    def build_vocab(self, sentences, min_count=10):\n",
    "        \"\"\" 各単語の出現回数の辞書を作成する\n",
    "        :param senteces(str) 文章\n",
    "        :param min_count(int) 最低カウント数(これ以下の単語は辞書に含めない)\n",
    "        \"\"\"\n",
    "        word_counter = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                word_counter[word] = word_counter.get(word, 0) + 1\n",
    "\n",
    "        # min_count回以上出現する単語のみ語彙に加える\n",
    "        for word, count in sorted(word_counter.items(), key=lambda x: -x[1]):\n",
    "            if count < min_count:\n",
    "                break\n",
    "            _id = len(self.word2id)\n",
    "            self.word2id.setdefault(word, _id)\n",
    "            self.id2word[_id] = word\n",
    "            \n",
    "def remove_choice_number(text):\n",
    "    '''文頭に選択肢番号がついている場合それを除く。\n",
    "    前処理で使うだけなのでこのファイルでは呼び出さない。別のファイルに移したい。\n",
    "    '''\n",
    "    remove_list = [\n",
    "        \"^ア \", \"^イ \", \"^ウ \", \"^エ \", \"^オ \", \"^1 \", \"^2 \", \"^3 \", \"^4 \", \"^5 \"\n",
    "    ]\n",
    "    for i, word in enumerate(remove_list):\n",
    "        text = re.sub(word, \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_symbol(text):\n",
    "    '''\n",
    "    入力されたテキストから句読点などの不要な記号をいくつか削除する。\n",
    "    '''\n",
    "    remove_list = [\n",
    "        ',', '.', '-', '、', '，', '。', '\\ufeff', '\\u3000', '「', '」', '（', '）',\n",
    "        '(', ')','\\n'\n",
    "    ]\n",
    "    for i, symbol in enumerate(remove_list):\n",
    "        text = text.replace(symbol, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_bos_eos(text):\n",
    "    '''\n",
    "    文章の先頭に<BOS>、<EOS>を加える。文末の改行コードの都合で<EOS>の直前にはスペースを入れていない。\n",
    "    '''\n",
    "    return \"<BOS> \" + text + \"<EOS>\"\n",
    "\n",
    "\n",
    "def replace_number(text):\n",
    "    '''textの数値表現を<Number>トークンに置き換える\n",
    "    textは分かち書きされていること\n",
    "    '''\n",
    "    new_text = \"\"\n",
    "    for word in text.split(' '):\n",
    "        if word.isnumeric():\n",
    "            new_text += \"<NUM> \"\n",
    "        elif word == \"<EOS>\":\n",
    "            new_text += \"<EOS>\"\n",
    "        else:\n",
    "            new_text += word + \" \"\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def isalpha(s):\n",
    "    '''\n",
    "    Args:\n",
    "        s:string\n",
    "    Returns:\n",
    "        bool:sが半角英字から成るかどうか\n",
    "    '''\n",
    "    alphaReg = re.compile(r'^[a-zA-Z]+$')\n",
    "    return alphaReg.match(s) is not None\n",
    "\n",
    "\n",
    "def replace_alphabet(text):\n",
    "    '''\n",
    "    Args:\n",
    "    text:分かち書きされた文。\n",
    "    Return:\n",
    "    textの数値表現をAに置き換える\n",
    "    '''\n",
    "    new_text = \"\"\n",
    "    for word in text.split(' '):\n",
    "        if isalpha(word):\n",
    "            new_text += \"A \"\n",
    "        elif word == \"<EOS>\":\n",
    "            new_text += word\n",
    "        else:\n",
    "            new_text += word + \" \"\n",
    "    return new_text\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "#########ここもまとめたい\n",
    "###\n",
    "#過去12年分の宅建の過去問\n",
    "takken = pd.read_csv(DATA_PATH+\"takken.csv\", encoding='utf-8')\n",
    "mondaishu = pd.read_csv(DATA_PATH+\"mondaishu.csv\", encoding='utf-8')\n",
    "nikken = pd.read_csv(DATA_PATH+\"nikken.csv\",encoding='utf-8')\n",
    "legal_mind = pd.read_csv(DATA_PATH+\"legal_mind.csv\",encoding='utf-8')\n",
    "\n",
    "#データをまとめる\n",
    "takken = takken[[\"Question\", \"Choice\"]]\n",
    "ocr = pd.concat([mondaishu,nikken,legal_mind],axis=0,ignore_index=True)\n",
    "ocr = ocr[[\"Wakati_Question\",\"Wakati_Choice\"]]\n",
    "ocr.columns = [\"Question\",\"Choice\"]\n",
    "\n",
    "#データをMeCabで分かち書きして、不要な文字の除去、BOS,EOSの追加を行う\n",
    "m = MeCab.Tagger(\"-Owakati\")\n",
    "takken = takken.applymap(remove_symbol)\n",
    "ocr = ocr.applymap(remove_symbol)\n",
    "takken = takken.applymap(m.parse)\n",
    "takken = pd.concat([takken,ocr],axis=0,ignore_index=True)\n",
    "takken = takken.applymap(remove_symbol)\n",
    "takken = takken.applymap(add_bos_eos)\n",
    "takken = takken.applymap(replace_number)\n",
    "takken = takken.applymap(replace_alphabet)\n",
    "print(\"data size is\",len(takken))\n",
    "#takken.head()\n",
    "\n",
    "\n",
    "################################################################\n",
    "#改良１\n",
    "#選択股と質問文で長さ指定してみる(本来これでうまくいくはずなのだが)\n",
    "#まずは同じ長さ→別の長さにすることを目指す\n",
    "max_length_question=100\n",
    "max_length_choice=100\n",
    "\n",
    "takken = takken[takken[\"Question\"].str.split(' ').apply(len) <= max_length_question]\n",
    "takken = takken[takken[\"Choice\"].str.split(' ').apply(len) <= max_length_choice]\n",
    "takken.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_question = []\n",
    "data_choice = []\n",
    "for i in range(len(takken)):\n",
    "    data_sentence_question = []\n",
    "    data_sentence_choice = []\n",
    "    for word in (takken.loc[i, 'Question']).split():\n",
    "        data_sentence_question.append(word)\n",
    "    data_question.append(data_sentence_question)\n",
    "    for word in (takken.loc[i, 'Choice']).split():\n",
    "        data_sentence_choice.append(word)\n",
    "    data_choice.append(data_sentence_choice)\n",
    "\n",
    "##############################################################\n",
    "#vocabに保存\n",
    "\"\"\"\n",
    "with open('question.vocab','wb') as f:\n",
    "    pickle.dump(question_words, f)\n",
    "with open('choice.vocab', 'wb') as g:\n",
    "    pickle.dump(choice_words, g)\n",
    "\n",
    "datasize = takken.shape[0]\n",
    "\"\"\"\n",
    "######################################################################\n",
    "MIN_COUNT = 1\n",
    "# 特殊なトークンを事前に定義します\n",
    "PAD_TOKEN = '<PAD>'\n",
    "BOS_TOKEN = '<S>'\n",
    "EOS_TOKEN = '</S>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "PAD = 0\n",
    "BOS = 1\n",
    "EOS = 2\n",
    "UNK = 3\n",
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    BOS_TOKEN: BOS,\n",
    "    EOS_TOKEN: EOS,\n",
    "    UNK_TOKEN: UNK,\n",
    "    }\n",
    "\n",
    "def sentence_to_ids(vocab, sentence):\n",
    "    ids = [vocab.word2id.get(word, UNK) for word in sentence]\n",
    "    ids = [BOS] + ids + [EOS]\n",
    "    return ids\n",
    "\n",
    "def pad_seq(sen, max_length):\n",
    "    if len(sen) <= max_length:\n",
    "        sen += [PAD] * (max_length - len(sen))\n",
    "    else:\n",
    "        sen = sen[:max_length]\n",
    "    return sen\n",
    "print(data_choice[0])\n",
    "vocab = Vocab(word2id=word2id)\n",
    "vocab.build_vocab(data_choice, min_count=MIN_COUNT)\n",
    "print(\"語彙数\\t:\", len(vocab.word2id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, batch_size, generated_num, output_file):\n",
    "    samples= []\n",
    "    for _ in range(int(generated_num / batch_size)):\n",
    "        sample = model.sample().cpu().data.numpy().tolist()\n",
    "        samples.extend(sample)\n",
    "\n",
    "    with open(output_file, 'w') as fout:\n",
    "        for sample in samples:\n",
    "            string = ' '.join([str(s) for s in sample]) + \"\\n\"\n",
    "            fout.write(string)\n",
    "            \n",
    "def compute_loss(model, data_loader, criterion, optimizer=None, is_train=True):\n",
    "    model.train(is_train)\n",
    "    total_loss = 0.\n",
    "    total_batches = 0.\n",
    "    \n",
    "    for batch_X, batch_Y in data_loader:\n",
    "        pred_Y = model(batch_X)\n",
    "        loss = criterion(pred_Y, batch_Y.contiguous().view(-1))\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "        \n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    data_loader.reset()\n",
    "    loss = total_loss / total_batches\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Hyper-parameters\n",
    "G_EMBEDDING_SIZE = 32 # 埋め込みベクトルの次元数\n",
    "G_HIDDEN_SIZE = 32 # LSTMの隠れ状態ベクトルの次元数\n",
    "G_MAX_LENGTH = 80 # 系列の長さ\n",
    "G_PRE_NUM_EPOCHS = 120 # 事前学習を行うエポック数\n",
    "#G_PRE_NUM_EPOCHS = 1\n",
    "G_BATCH_SIZE = 64 # バッチサイズ\n",
    "\n",
    "# Discriminator Hyper-parameters\n",
    "D_EMBEDDING_SIZE = 64\n",
    "D_FILTER_SIZES = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20] # CNNに使うフィルターのサイズ\n",
    "D_NUM_FILTERS = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160] # CNNに使うフィルターの数\n",
    "D_DROPOUT_PROB = 0.75 # Dropoutの確率\n",
    "D_PRE_NUM_EPOCHS = 50\n",
    "#D_PRE_NUM_EPOCHS = 1\n",
    "D_BATCH_SIZE = 64\n",
    "\n",
    "# Basic Training Parameters\n",
    "ADV_N_BATCHES = 200 # 敵対的学習を行うエポック数\n",
    "#ADV_N_BATCHES = 1\n",
    "POSITIVE_FILE = './save/real_data.txt'\n",
    "NEGATIVE_FILE = './save/generator_sample.txt'\n",
    "GENERATED_NUM = 10000\n",
    "#GENERATED_NUM = 1000 # 敵対的学習のためにGeneratorに出力させるサンプル数\n",
    "\n",
    "# 保存用ディレクトリがなければ作成する\n",
    "if not os.path.exists('./save'):\n",
    "    os.mkdir('./save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_text = []\n",
    "for sen in data_choice:\n",
    "    id_sen =sentence_to_ids(vocab, sen)\n",
    "    id_sen = pad_seq(id_sen, G_MAX_LENGTH)\n",
    "    id_text.append(id_sen)\n",
    "    \n",
    "with open(POSITIVE_FILE, 'w') as f:\n",
    "    for id_sen in id_text:\n",
    "        f.write(' '.join([str(w) for w in id_sen])+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, GenLoss:5.6965\n",
      "Epoch:2, GenLoss:3.6070\n",
      "Epoch:3, GenLoss:3.3424\n",
      "Epoch:4, GenLoss:3.1393\n",
      "Epoch:5, GenLoss:2.9169\n",
      "Epoch:6, GenLoss:2.7238\n",
      "Epoch:7, GenLoss:2.5752\n",
      "Epoch:8, GenLoss:2.4650\n",
      "Epoch:9, GenLoss:2.3790\n",
      "Epoch:10, GenLoss:2.3088\n",
      "Epoch:11, GenLoss:2.2498\n",
      "Epoch:12, GenLoss:2.1991\n",
      "Epoch:13, GenLoss:2.1551\n",
      "Epoch:14, GenLoss:2.1163\n",
      "Epoch:15, GenLoss:2.0814\n",
      "Epoch:16, GenLoss:2.0497\n",
      "Epoch:17, GenLoss:2.0206\n",
      "Epoch:18, GenLoss:1.9937\n",
      "Epoch:19, GenLoss:1.9685\n",
      "Epoch:20, GenLoss:1.9452\n",
      "Epoch:21, GenLoss:1.9233\n",
      "Epoch:22, GenLoss:1.9029\n",
      "Epoch:23, GenLoss:1.8837\n",
      "Epoch:24, GenLoss:1.8658\n",
      "Epoch:25, GenLoss:1.8489\n",
      "Epoch:26, GenLoss:1.8330\n",
      "Epoch:27, GenLoss:1.8179\n",
      "Epoch:28, GenLoss:1.8034\n",
      "Epoch:29, GenLoss:1.7895\n",
      "Epoch:30, GenLoss:1.7763\n",
      "Epoch:31, GenLoss:1.7636\n",
      "Epoch:32, GenLoss:1.7514\n",
      "Epoch:33, GenLoss:1.7397\n",
      "Epoch:34, GenLoss:1.7285\n",
      "Epoch:35, GenLoss:1.7176\n",
      "Epoch:36, GenLoss:1.7070\n",
      "Epoch:37, GenLoss:1.6969\n",
      "Epoch:38, GenLoss:1.6872\n",
      "Epoch:39, GenLoss:1.6777\n",
      "Epoch:40, GenLoss:1.6687\n",
      "Epoch:41, GenLoss:1.6599\n",
      "Epoch:42, GenLoss:1.6512\n",
      "Epoch:43, GenLoss:1.6430\n",
      "Epoch:44, GenLoss:1.6349\n",
      "Epoch:45, GenLoss:1.6273\n",
      "Epoch:46, GenLoss:1.6197\n",
      "Epoch:47, GenLoss:1.6124\n",
      "Epoch:48, GenLoss:1.6054\n",
      "Epoch:49, GenLoss:1.5985\n",
      "Epoch:50, GenLoss:1.5918\n",
      "Epoch:51, GenLoss:1.5853\n",
      "Epoch:52, GenLoss:1.5788\n",
      "Epoch:53, GenLoss:1.5727\n",
      "Epoch:54, GenLoss:1.5667\n",
      "Epoch:55, GenLoss:1.5607\n",
      "Epoch:56, GenLoss:1.5548\n",
      "Epoch:57, GenLoss:1.5493\n",
      "Epoch:58, GenLoss:1.5438\n",
      "Epoch:59, GenLoss:1.5384\n",
      "Epoch:60, GenLoss:1.5333\n",
      "Epoch:61, GenLoss:1.5281\n",
      "Epoch:62, GenLoss:1.5232\n",
      "Epoch:63, GenLoss:1.5183\n",
      "Epoch:64, GenLoss:1.5136\n",
      "Epoch:65, GenLoss:1.5089\n",
      "Epoch:66, GenLoss:1.5041\n",
      "Epoch:67, GenLoss:1.4995\n",
      "Epoch:68, GenLoss:1.4951\n",
      "Epoch:69, GenLoss:1.4908\n",
      "Epoch:70, GenLoss:1.4866\n",
      "Epoch:71, GenLoss:1.4824\n",
      "Epoch:72, GenLoss:1.4783\n",
      "Epoch:73, GenLoss:1.4743\n",
      "Epoch:74, GenLoss:1.4704\n",
      "Epoch:75, GenLoss:1.4666\n",
      "Epoch:76, GenLoss:1.4628\n",
      "Epoch:77, GenLoss:1.4592\n",
      "Epoch:78, GenLoss:1.4556\n",
      "Epoch:79, GenLoss:1.4521\n",
      "Epoch:80, GenLoss:1.4487\n",
      "Epoch:81, GenLoss:1.4452\n",
      "Epoch:82, GenLoss:1.4420\n",
      "Epoch:83, GenLoss:1.4388\n",
      "Epoch:84, GenLoss:1.4356\n",
      "Epoch:85, GenLoss:1.4326\n",
      "Epoch:86, GenLoss:1.4295\n",
      "Epoch:87, GenLoss:1.4266\n",
      "Epoch:88, GenLoss:1.4236\n",
      "Epoch:89, GenLoss:1.4208\n",
      "Epoch:90, GenLoss:1.4179\n",
      "Epoch:91, GenLoss:1.4152\n",
      "Epoch:92, GenLoss:1.4125\n",
      "Epoch:93, GenLoss:1.4099\n",
      "Epoch:94, GenLoss:1.4073\n",
      "Epoch:95, GenLoss:1.4046\n",
      "Epoch:96, GenLoss:1.4021\n",
      "Epoch:97, GenLoss:1.3996\n",
      "Epoch:98, GenLoss:1.3971\n",
      "Epoch:99, GenLoss:1.3947\n",
      "Epoch:100, GenLoss:1.3923\n",
      "Epoch:101, GenLoss:1.3900\n",
      "Epoch:102, GenLoss:1.3878\n",
      "Epoch:103, GenLoss:1.3855\n",
      "Epoch:104, GenLoss:1.3832\n",
      "Epoch:105, GenLoss:1.3811\n",
      "Epoch:106, GenLoss:1.3790\n",
      "Epoch:107, GenLoss:1.3768\n",
      "Epoch:108, GenLoss:1.3747\n",
      "Epoch:109, GenLoss:1.3727\n",
      "Epoch:110, GenLoss:1.3706\n",
      "Epoch:111, GenLoss:1.3686\n",
      "Epoch:112, GenLoss:1.3666\n",
      "Epoch:113, GenLoss:1.3647\n",
      "Epoch:114, GenLoss:1.3628\n",
      "Epoch:115, GenLoss:1.3609\n",
      "Epoch:116, GenLoss:1.3590\n",
      "Epoch:117, GenLoss:1.3571\n",
      "Epoch:118, GenLoss:1.3554\n",
      "Epoch:119, GenLoss:1.3536\n",
      "Epoch:120, GenLoss:1.3518\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab.word2id)\n",
    "generator = Generator(vocab_size, G_BATCH_SIZE, G_EMBEDDING_SIZE, G_HIDDEN_SIZE, G_MAX_LENGTH).to(device)\n",
    "discriminator = Discriminator(vocab_size, D_BATCH_SIZE, D_EMBEDDING_SIZE,\n",
    "                D_NUM_FILTERS, D_FILTER_SIZES, dropout_prob=D_DROPOUT_PROB).to(device)\n",
    "\n",
    "gen_data_loader = GenDataLoader(POSITIVE_FILE, G_BATCH_SIZE)\n",
    "gen_criterion = nn.NLLLoss()\n",
    "gen_optimizer = optim.Adam(generator.parameters())\n",
    "for epoch_id in range(1, G_PRE_NUM_EPOCHS + 1):\n",
    "    gen_data_loader.reset()\n",
    "    train_loss = compute_loss(\n",
    "        generator, gen_data_loader, gen_criterion, optimizer=gen_optimizer, is_train=True)\n",
    "    print(\"Epoch:{}, GenLoss:{:.4f}\".format(epoch_id, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, DisLoss: 0.1438\n",
      "Epoch:2, DisLoss: 0.0005\n",
      "Epoch:3, DisLoss: 0.0001\n",
      "Epoch:4, DisLoss: 0.0000\n",
      "Epoch:5, DisLoss: 0.0086\n",
      "Epoch:6, DisLoss: 0.0010\n",
      "Epoch:7, DisLoss: 0.0000\n",
      "Epoch:8, DisLoss: 0.0000\n",
      "Epoch:9, DisLoss: 0.0000\n",
      "Epoch:10, DisLoss: 0.0000\n",
      "Epoch:11, DisLoss: 0.0000\n",
      "Epoch:12, DisLoss: 0.0000\n",
      "Epoch:13, DisLoss: 0.0000\n",
      "Epoch:14, DisLoss: 0.0000\n",
      "Epoch:15, DisLoss: 0.0000\n",
      "Epoch:16, DisLoss: 0.0000\n",
      "Epoch:17, DisLoss: 0.0000\n",
      "Epoch:18, DisLoss: 0.0000\n",
      "Epoch:19, DisLoss: 0.0217\n",
      "Epoch:20, DisLoss: 0.0009\n",
      "Epoch:21, DisLoss: 0.0002\n",
      "Epoch:22, DisLoss: 0.0007\n",
      "Epoch:23, DisLoss: 0.0022\n",
      "Epoch:24, DisLoss: 0.0000\n",
      "Epoch:25, DisLoss: 0.0005\n",
      "Epoch:26, DisLoss: 0.0000\n",
      "Epoch:27, DisLoss: 0.0091\n",
      "Epoch:28, DisLoss: 0.0147\n",
      "Epoch:29, DisLoss: 0.0034\n",
      "Epoch:30, DisLoss: 0.0000\n",
      "Epoch:31, DisLoss: 0.0169\n",
      "Epoch:32, DisLoss: 0.0007\n",
      "Epoch:33, DisLoss: 0.0014\n",
      "Epoch:34, DisLoss: 0.0000\n",
      "Epoch:35, DisLoss: 0.0000\n",
      "Epoch:36, DisLoss: 0.0000\n",
      "Epoch:37, DisLoss: 0.0000\n",
      "Epoch:38, DisLoss: 0.0000\n",
      "Epoch:39, DisLoss: 0.0000\n",
      "Epoch:40, DisLoss: 0.0010\n",
      "Epoch:41, DisLoss: 0.0000\n",
      "Epoch:42, DisLoss: 0.0000\n",
      "Epoch:43, DisLoss: 0.0000\n",
      "Epoch:44, DisLoss: 0.0000\n",
      "Epoch:45, DisLoss: 0.0029\n",
      "Epoch:46, DisLoss: 0.0000\n",
      "Epoch:47, DisLoss: 0.0000\n",
      "Epoch:48, DisLoss: 0.0000\n",
      "Epoch:49, DisLoss: 0.0000\n",
      "Epoch:50, DisLoss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "dis_criterion = nn.NLLLoss()\n",
    "dis_optimizer = optim.Adam(discriminator.parameters())\n",
    "for epoch_id in range(1, D_PRE_NUM_EPOCHS + 1):\n",
    "    generate_samples(generator, D_BATCH_SIZE, GENERATED_NUM, NEGATIVE_FILE)\n",
    "    dis_data_loader = DisDataLoader(POSITIVE_FILE, NEGATIVE_FILE, D_BATCH_SIZE)\n",
    "    for _ in range(3):\n",
    "        loss = compute_loss(\n",
    "            discriminator, dis_data_loader, dis_criterion, optimizer=dis_optimizer, is_train=True)\n",
    "        print(\"Epoch:{}, DisLoss: {:.4f}\".format(epoch_id, loss))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/develop/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1, TrainLoss:0.0003\n",
      "Batch: 2, TrainLoss:0.0018\n",
      "Batch: 3, TrainLoss:0.0032\n",
      "Batch: 4, TrainLoss:0.0011\n",
      "Batch: 5, TrainLoss:0.0000\n",
      "Batch: 6, TrainLoss:0.0000\n",
      "Batch: 7, TrainLoss:0.0000\n",
      "Batch: 8, TrainLoss:0.0029\n",
      "Batch: 9, TrainLoss:0.0000\n",
      "Batch: 10, TrainLoss:0.0003\n",
      "Batch: 11, TrainLoss:0.0010\n",
      "Batch: 12, TrainLoss:0.0000\n",
      "Batch: 13, TrainLoss:0.0000\n",
      "Batch: 14, TrainLoss:0.0000\n",
      "Batch: 15, TrainLoss:0.0007\n",
      "Batch: 16, TrainLoss:0.0000\n"
     ]
    }
   ],
   "source": [
    "rollout = Rollout(generator, 0.8)\n",
    "\n",
    "gen_gan_criterion = nn.NLLLoss(reduce=False)\n",
    "gen_gan_optimizer = optim.Adam(generator.parameters())\n",
    "dis_criterion = nn.NLLLoss()\n",
    "dis_optimizer = optim.Adam(discriminator.parameters())\n",
    "for batch_id in range(1, ADV_N_BATCHES + 1):\n",
    "    for it in range(1):\n",
    "        samples = generator.sample()\n",
    "        start_tokens = torch.empty(G_BATCH_SIZE, 1).fill_(BOS).type(torch.long)\n",
    "        start_tokens = start_tokens.to(device)\n",
    "        inputs = torch.cat([start_tokens, samples], dim= 1)[:, :-1].contiguous()\n",
    "        targets = samples.contiguous().view((-1,))\n",
    "        \n",
    "        rewards = rollout.get_reward(samples, 16, discriminator)\n",
    "        rewards = torch.Tensor(rewards).contiguous().view((-1)).to(device)\n",
    "        log_prob = generator.forward(inputs)\n",
    "        train_loss = gen_gan_criterion(log_prob, targets)\n",
    "        train_loss = (train_loss*rewards).mean()\n",
    "        \n",
    "        gen_gan_optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        gen_gan_optimizer.step()\n",
    "        \n",
    "    print('Batch: {}, TrainLoss:{:.4f}'.format(batch_id, train_loss.item()))\n",
    "    \n",
    "    rollout.update_params()\n",
    "    \n",
    "    for _ in range(4):\n",
    "        generate_samples(generator, G_BATCH_SIZE, GENERATED_NUM, NEGATIVE_FILE)\n",
    "        dis_data_loader = DisDataLoader(POSITIVE_FILE, NEGATIVE_FILE, D_BATCH_SIZE)\n",
    "        for _ in range(2):\n",
    "            loss = compute_loss(discriminator, dis_data_loader,dis_criterion,\n",
    "                               optimizer=dis_optimizer, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim,\n",
    "                 sequence_length):\n",
    "        \"\"\"\n",
    "        :param num_emb: int, 語彙の総数\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param emb_dim: int, 埋め込みベクトルの次元数\n",
    "        :param hidden_dim: int, 隠れ状態ベクトルの次元数\n",
    "        :param sequence_length: int, 入出力系列の長さ\n",
    "        \"\"\"\n",
    "        super(TextGenerator, self).__init__()\n",
    "        self.num_emb = num_emb\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # 埋め込み層\n",
    "        self.embedding = nn.Embedding(self.num_emb, self.emb_dim)\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(self.emb_dim, self.hidden_dim, 1, batch_first=True)\n",
    "        # 全結合層\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.num_emb)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        ターゲット系列を全時刻での入力として出力を計算\n",
    "        :param inputs: torch.Tensor, (batch_size, sequence_length)\n",
    "        :return outputs: torch.Tensor, (batch_size*sequence_length, num_emb)\n",
    "        \"\"\"\n",
    "        N = inputs.size(0) # batch_size\n",
    "        embed = self.embedding(inputs) # (batch_size, sequence_length, emb_dim)\n",
    "        h0, c0 = self.init_hidden(N) # 隠れ状態ベクトルの初期化\n",
    "        h = (h0, c0)\n",
    "        self.lstm.flatten_parameters()\n",
    "        hidden, h = self.lstm(embed, h) # hidden:(batch_size, sequence_length, hidden_dim)\n",
    "        lin = self.linear(hidden) # (batch_size, sequence_length, num_emb)\n",
    "        outputs = F.log_softmax(lin, dim=-1) # (batch_size, sequence_length, num_emb)\n",
    "        outputs = outputs.view(-1, self.num_emb) # (batch_size * sequence_length, num_emb)\n",
    "        return outputs\n",
    "\n",
    "    def step(self, x, h, c):\n",
    "        \"\"\"\n",
    "        時刻をtからt+1に1つだけ進めます\n",
    "        :param x: torch.Tensor, 時刻tの出力かつ時刻t+1の入力\n",
    "        :param h, c: torch.Tensor, 時刻tの隠れ状態ベクトル\n",
    "        :return pred: torch.Tensor, 時刻t+1の出力\n",
    "        :return h, c: torch.Tensor, 時刻t+1の隠れ状態ベクトル\n",
    "        \"\"\"\n",
    "        embed = self.embedding(x) # embed:(batch_size, 1, emb_dim)\n",
    "        self.lstm.flatten_parameters()\n",
    "        y, (h, c) = self.lstm(embed, (h, c)) # y:(batch_size, 1, hidden_dim)\n",
    "        pred = F.softmax(self.linear(y), dim=-1) # (batch_size, 1, num_emb)\n",
    "        return pred, h, c\n",
    "\n",
    "    def sample(self, x=None):\n",
    "        \"\"\"\n",
    "        Generaterでサンプリングするメソッド\n",
    "        :param x: None or torch.Tensor\n",
    "        :param output: torch.Tensor, (batch_size, sequence_length)\n",
    "        \"\"\"\n",
    "        flag = False # 時刻0から始める(True)か否か(False)\n",
    "        if x is None:\n",
    "            flag = True\n",
    "        if flag:\n",
    "            x = torch.empty(self.batch_size, 1).fill_(1).long().to(device) # BOS == 1\n",
    "        h, c = self.init_hidden(self.batch_size)\n",
    "\n",
    "        samples = []\n",
    "        if flag:\n",
    "            for i in range(self.sequence_length):\n",
    "                output, h, c = self.step(x, h, c) # output:(batch_size, 1, num_emb)\n",
    "                output = output.squeeze(1) # (batch_size, num_emb)\n",
    "                x = output.multinomial(1) # (batch_size, 1), 次の時刻の入力を多項分布からサンプリング\n",
    "                samples.append(x)\n",
    "        else:\n",
    "            given_len = x.size(1)\n",
    "            lis = x.chunk(x.size(1), dim=1) # sequence_length方向に分割\n",
    "            for i in range(given_len):\n",
    "                output, h, c = self.step(lis[i], h, c)\n",
    "                samples.append(lis[i])\n",
    "            output = output.squeeze(1)\n",
    "            x = output.multinomial(1)\n",
    "            for i in range(given_len, self.sequence_length):\n",
    "                samples.append(x)\n",
    "                output, h, c = self.step(x, h, c)\n",
    "                output = output.squeeze(1)\n",
    "                x = output.multinomial(1)\n",
    "        output = torch.cat(samples, dim=1)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self, N):\n",
    "        \"\"\"\n",
    "        LSTMの隠れ状態ベクトルを初期化します。\n",
    "        :param N: int, ミニバッチのサイズ\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(1, N, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(1, N, self.hidden_dim).to(device)\n",
    "        return h0, c0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(vocab, ids):\n",
    "    # IDのリストを単語のリストに変換する\n",
    "    return [vocab.id2word[int(_id)] for _id in ids]\n",
    "\n",
    "\n",
    "for line in open(NEGATIVE_FILE).readlines():\n",
    "    ids_line = []\n",
    "    for word in line[:-1].split(' '):\n",
    "        ids_line.append(word)\n",
    "    print(ids_to_sentence(vocab, ids_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
